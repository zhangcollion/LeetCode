{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_data(tmp_x1,tmp_x2):\n",
    "    y = tmp_x1**2 +  2*tmp_x2\n",
    "    return y\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch import  FloatTensor\n",
    "class UserDataset(Dataset):\n",
    "    def __init__(self, doc=\"dataset\"):\n",
    "        self.doc = doc\n",
    "        x1 = list(range(0,100,3))\n",
    "        self.x1 =  FloatTensor(x1).view(-1, 1)\n",
    "        x2 = list(range(0,100,3))\n",
    "        self.x2 =  FloatTensor(x2).view(-1, 1)\n",
    "        y = list(map(y_data,x1,x2))\n",
    "        self.y = FloatTensor(y).view(-1, 1)\n",
    " \n",
    "    def __getitem__(self,index):\n",
    "        return self.x1[index], self.x2[index],self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x4c25ac8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 随机种子设置，每次执行后再训练模型可以得到相同loss\n",
    "random.seed(3407)\n",
    "np.random.seed(3407)\n",
    "torch.manual_seed(3407) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "user_dataset = UserDataset()\n",
    "user_loader = DataLoader(user_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class reg_model(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim=1):\n",
    "        super(reg_model, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "#         self.model = nn.Sequential(nn.Linear(in_dim, 16), nn.ReLU(),nn.Linear(16, out_dim))\n",
    "        self.fc1 = nn.Linear(2, 16)\n",
    "        self.act   = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(16, out_dim)\n",
    "        self.register_buffer('mean', torch.zeros(out_dim), persistent=False)\n",
    "        \n",
    "    def forward(self,x1, x2):\n",
    "#         y = self.model(x)\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        y = self.fc2(x) + self.mean\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight \t tensor([[-0.2694,  0.1531],\n",
      "        [-0.2477,  0.0217],\n",
      "        [-0.6537, -0.0335],\n",
      "        [ 0.6686, -0.4134],\n",
      "        [ 0.1201,  0.5221],\n",
      "        [-0.5310, -0.0166],\n",
      "        [ 0.1456, -0.1204],\n",
      "        [ 0.1694,  0.5028],\n",
      "        [-0.2763, -0.2221],\n",
      "        [ 0.7702, -0.2230],\n",
      "        [-0.1797,  0.6505],\n",
      "        [ 0.0204,  0.3966],\n",
      "        [ 0.1710, -0.2726],\n",
      "        [ 0.6551,  0.6226],\n",
      "        [-0.0836, -0.1713],\n",
      "        [ 0.3487, -0.0654]])\n",
      "fc1.bias \t tensor([-0.1027,  0.4742, -0.1861,  0.4799,  0.2508, -0.3902, -0.5208, -0.2870,\n",
      "         0.3514,  0.1034, -0.4733,  0.8011, -0.3259, -0.1752, -0.6088,  0.5182])\n",
      "fc2.weight \t tensor([[-0.0543, -0.2161,  0.1235,  0.1782,  0.0935,  0.0568,  0.0259,  0.1301,\n",
      "         -0.0478,  0.2728,  0.3644,  0.3564,  0.0979,  0.3294, -0.1119,  0.2285]])\n",
      "fc2.bias \t tensor([0.2832])\n",
      "------------------loss------------------ 50134450.0\n",
      "fc1.weight \t tensor([[-0.2694,  0.1531],\n",
      "        [-0.2477,  0.0217],\n",
      "        [-0.6537, -0.0335],\n",
      "        [ 0.8202, -0.2618],\n",
      "        [ 0.1324,  0.5345],\n",
      "        [-0.5310, -0.0166],\n",
      "        [ 0.0207, -0.2453],\n",
      "        [ 0.2738,  0.6072],\n",
      "        [-0.2763, -0.2221],\n",
      "        [ 0.9401, -0.0532],\n",
      "        [-0.0076,  0.8227],\n",
      "        [ 0.1924,  0.5686],\n",
      "        [ 0.1710, -0.2726],\n",
      "        [ 0.8269,  0.7944],\n",
      "        [-0.0836, -0.1713],\n",
      "        [ 0.5050,  0.0909]])\n",
      "fc1.bias \t tensor([-0.1027,  0.5963, -0.1861,  0.6346,  0.2681, -0.3902, -0.6423, -0.1832,\n",
      "         0.3910,  0.2808, -0.2930,  0.9812, -0.3259,  0.0044, -0.6088,  0.6806])\n",
      "fc2.weight \t tensor([[-0.0543, -0.3476,  0.1235,  0.3613,  0.2642,  0.0568,  0.1704,  0.3055,\n",
      "         -0.1916,  0.4426,  0.5307,  0.5205,  0.0979,  0.5027, -0.1119,  0.3882]])\n",
      "fc2.bias \t tensor([0.4636])\n",
      "------------------loss------------------ 403828.16\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "import torch\n",
    "model = reg_model(1,1)\n",
    "criterior = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "writer = SummaryWriter(log_dir=r\"G:\\ML\\PYLearn\\jupyter\\log\")\n",
    "for i in range(2):\n",
    "    for data in user_loader:\n",
    "        x1,x2, y = data\n",
    "        optimizer.zero_grad()\n",
    "        pred_y = model(x1,x2)\n",
    "        loss = criterior(pred_y, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    for param_tensor in model.state_dict():  # 字典的遍历默认是遍历 key，所以param_tensor实际上是键值\n",
    "        print(param_tensor, '\\t', model.state_dict()[param_tensor])\n",
    "    print(\"------------------loss------------------\", loss.detach().numpy())\n",
    "#         writer.add_scalar(\"train_loss\", loss,i)\n",
    "#     for name, parms in model.named_parameters(): \n",
    "#         print('-->name:', name, '-->grad_requirs:',parms, \\\n",
    "#          ' -->grad_value:',parms.grad)\n",
    "#     for param_tensor in model.state_dict():  # 字典的遍历默认是遍历 key，所以param_tensor实际上是键值\n",
    "#         print(param_tensor, '\\t', model.state_dict()[param_tensor])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(model, (x1,x2))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc1.weight', tensor([[ 0.4831,  0.4410],\n",
       "                      [-0.0554,  0.9087],\n",
       "                      [-0.4157, -0.2391],\n",
       "                      [ 0.5926,  0.3606],\n",
       "                      [ 0.1151, -0.4883],\n",
       "                      [ 0.0661, -0.4727],\n",
       "                      [-0.0869,  0.9922],\n",
       "                      [-0.3671,  0.1279],\n",
       "                      [ 0.8341,  0.3113],\n",
       "                      [ 0.1338, -0.3689],\n",
       "                      [ 0.6387,  0.2660],\n",
       "                      [ 0.1682,  0.8132],\n",
       "                      [ 0.3095,  0.7375],\n",
       "                      [-0.1231, -0.3312],\n",
       "                      [-0.3302, -0.3534],\n",
       "                      [-0.5844, -0.1470]])),\n",
       "             ('fc1.bias',\n",
       "              tensor([ 0.5340, -0.0266,  0.1758,  0.0018, -0.8335,  0.5722, -0.0641, -0.2288,\n",
       "                      -0.3111, -0.6204,  0.9452,  0.3559,  0.0180,  0.6714,  0.0464, -0.3840])),\n",
       "             ('fc2.weight',\n",
       "              tensor([[ 0.3667,  0.4421, -0.2800,  0.5677,  0.1789, -0.1990,  0.5096, -0.2069,\n",
       "                        0.4173,  0.0633,  0.5693,  0.4894,  0.5333, -0.3175, -0.1380,  0.0725]])),\n",
       "             ('fc2.bias', tensor([0.2913]))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(0,100))\n",
    "y = list(map(y_data, x,x))\n",
    "x = torch.unsqueeze(FloatTensor(x),dim=1)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_y = model(x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x62751ba8>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x,y)\n",
    "plt.plot(x, pred_y.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "A kind of Tensor that is to be considered a module parameter.\n",
       "\n",
       "Parameters are :class:`~torch.Tensor` subclasses, that have a\n",
       "very special property when used with :class:`Module` s - when they're\n",
       "assigned as Module attributes they are automatically added to the list of\n",
       "its parameters, and will appear e.g. in :meth:`~Module.parameters` iterator.\n",
       "Assigning a Tensor doesn't have such effect. This is because one might\n",
       "want to cache some temporary state, like last hidden state of the RNN, in\n",
       "the model. If there was no such class as :class:`Parameter`, these\n",
       "temporaries would get registered too.\n",
       "\n",
       "Args:\n",
       "    data (Tensor): parameter tensor.\n",
       "    requires_grad (bool, optional): if the parameter requires gradient. See\n",
       "        :ref:`excluding-subgraphs` for more details. Default: `True`\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\programdata\\anaconda3\\lib\\site-packages\\torch\\nn\\parameter.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.Parameter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_cat(nn.Module):\n",
    "    def __init__(self, data, kpi_model,idx):\n",
    "        super(model_cat, self).__init__()\n",
    "        self.x = nn.Parameter(data, requires_grad=True)\n",
    "        self.idx = idx\n",
    "        self.model = kpi_model\n",
    "        \n",
    "    def forward(self, x, z):\n",
    "        out = self.model(self.x[self.idx,:], z)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n"
     ]
    }
   ],
   "source": [
    "tmp_data = torch.ones((200,1))\n",
    "cat_net = model_cat(tmp_data, model, np.arange(4, 104))\n",
    "optimizer = optim.Adam(params=[cat_net.x], lr=0.01)\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    loss = cat_net(tmp_data, x).mean()\n",
    "    loss.backward()\n",
    "    \n",
    "#     print(cat_net.x.grad)\n",
    "    if i == 1:\n",
    "        print('-----')\n",
    "        optimizer.step()\n",
    "    cat_net.x.grad = torch.ones((200,1))\n",
    "    optimizer.step()\n",
    "#     print(cat_net.x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "fc1.weight \t torch.Size([16, 1])\n",
      "fc1.bias \t torch.Size([16])\n",
      "fc2.weight \t torch.Size([1, 16])\n",
      "fc2.bias \t torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Returns the state of the optimizer as a :class:`dict`.\n",
       "\n",
       "It contains two entries:\n",
       "\n",
       "* state - a dict holding current optimization state. Its content\n",
       "    differs between optimizer classes.\n",
       "* param_groups - a dict containing all parameter groups\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\programdata\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\n",
       "\u001b[1;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer.state_dict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer's state_dict:\n",
      "state \t {0: {'step': 6000, 'exp_avg': tensor([[ 3.4952e+04],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 5.6052e-45],\n",
      "        [ 2.2232e+04],\n",
      "        [-8.6573e+04],\n",
      "        [ 4.2950e+04],\n",
      "        [ 2.2401e+04],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [-1.0213e+05],\n",
      "        [-1.3310e+05],\n",
      "        [ 2.1657e+04],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00]]), 'exp_avg_sq': tensor([[1.7020e+10],\n",
      "        [0.0000e+00],\n",
      "        [0.0000e+00],\n",
      "        [3.4815e+04],\n",
      "        [9.8124e+09],\n",
      "        [1.4238e+11],\n",
      "        [2.4256e+10],\n",
      "        [9.9775e+09],\n",
      "        [0.0000e+00],\n",
      "        [0.0000e+00],\n",
      "        [0.0000e+00],\n",
      "        [1.4763e+11],\n",
      "        [3.3066e+11],\n",
      "        [9.3116e+09],\n",
      "        [0.0000e+00],\n",
      "        [0.0000e+00]])}, 1: {'step': 6000, 'exp_avg': tensor([ 3.8946e+02,  0.0000e+00,  0.0000e+00,  5.6052e-45,  1.9953e+02,\n",
      "        -8.0688e+02,  4.7163e+02,  2.0104e+02,  0.0000e+00,  0.0000e+00,\n",
      "         5.6052e-45, -2.3568e+03, -1.2409e+03,  1.9438e+02,  0.0000e+00,\n",
      "         5.6052e-45]), 'exp_avg_sq': tensor([3.2896e+06, 0.0000e+00, 0.0000e+00, 6.0827e+00, 2.1217e+06, 3.1862e+07,\n",
      "        4.5085e+06, 2.1576e+06, 0.0000e+00, 0.0000e+00, 6.7008e-12, 1.0664e+08,\n",
      "        7.3869e+07, 2.0137e+06, 0.0000e+00, 4.3452e-09])}, 2: {'step': 6000, 'exp_avg': tensor([[ 5.8271e+03,  0.0000e+00,  0.0000e+00, -5.6052e-45,  1.8517e+04,\n",
      "         -2.6816e+02,  5.1527e+03,  1.7855e+04,  0.0000e+00,  0.0000e+00,\n",
      "          5.6052e-45, -3.8368e+02, -3.0490e+02,  1.8311e+04,  0.0000e+00,\n",
      "          5.6052e-45]]), 'exp_avg_sq': tensor([[1.0674e+09, 0.0000e+00, 0.0000e+00, 7.8760e+03, 6.3724e+09, 8.9297e+07,\n",
      "         5.9840e+08, 5.8711e+09, 0.0000e+00, 0.0000e+00, 1.1511e-10, 1.4873e+07,\n",
      "         8.3672e+07, 6.2335e+09, 0.0000e+00, 3.6128e-09]])}, 3: {'step': 6000, 'exp_avg': tensor([37.3642]), 'exp_avg_sq': tensor([74975.8750])}}\n",
      "param_groups \t [{'lr': 0.1, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3]}]\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': 1000,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, r\"G:\\ML\\PYLearn\\jupyter\\log\\model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(r\"G:\\ML\\PYLearn\\jupyter\\log\\model.pth\")\n",
    "# checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fc1.weight', Parameter containing:\n",
       "  tensor([[ 0.8607],\n",
       "          [-0.4612],\n",
       "          [ 1.1627],\n",
       "          [ 0.6596],\n",
       "          [-0.1963],\n",
       "          [-0.2116],\n",
       "          [-0.9390],\n",
       "          [-0.6986],\n",
       "          [ 1.1903],\n",
       "          [-0.2990],\n",
       "          [-0.2648],\n",
       "          [ 1.0166],\n",
       "          [ 0.5990],\n",
       "          [ 0.3458],\n",
       "          [ 1.3209],\n",
       "          [-0.2365]], requires_grad=True)), ('fc1.bias', Parameter containing:\n",
       "  tensor([-0.5419, -0.1460,  1.1332, -0.5549, -1.1715, -0.0499,  0.4957, -0.0812,\n",
       "           0.3717, -0.2156, -0.5980,  0.3973, -0.7329, -0.0947,  0.6900, -0.8912],\n",
       "         requires_grad=True)), ('fc2.weight', Parameter containing:\n",
       "  tensor([[ 0.3370, -0.0976,  0.4735,  0.1556,  0.1632,  0.1524, -0.1927,  0.1632,\n",
       "            0.4725,  0.0417, -0.0498,  0.3723,  0.1891,  0.3840,  0.5329,  0.0873]],\n",
       "         requires_grad=True)), ('fc2.bias', Parameter containing:\n",
       "  tensor([0.3488], requires_grad=True))]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.8607],\n",
       "         [-0.4612],\n",
       "         [ 1.1627],\n",
       "         [ 0.6596],\n",
       "         [-0.1963],\n",
       "         [-0.2116],\n",
       "         [-0.9390],\n",
       "         [-0.6986],\n",
       "         [ 1.1903],\n",
       "         [-0.2990],\n",
       "         [-0.2648],\n",
       "         [ 1.0166],\n",
       "         [ 0.5990],\n",
       "         [ 0.3458],\n",
       "         [ 1.3209],\n",
       "         [-0.2365]], requires_grad=True), Parameter containing:\n",
       " tensor([-0.5419, -0.1460,  1.1332, -0.5549, -1.1715, -0.0499,  0.4957, -0.0812,\n",
       "          0.3717, -0.2156, -0.5980,  0.3973, -0.7329, -0.0947,  0.6900, -0.8912],\n",
       "        requires_grad=True), Parameter containing:\n",
       " tensor([[ 0.3370, -0.0976,  0.4735,  0.1556,  0.1632,  0.1524, -0.1927,  0.1632,\n",
       "           0.4725,  0.0417, -0.0498,  0.3723,  0.1891,  0.3840,  0.5329,  0.0873]],\n",
       "        requires_grad=True), Parameter containing:\n",
       " tensor([0.3488], requires_grad=True)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### profiler to analyze the execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "        nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(ngf*8),\n",
    "        nn.ReLU(True),\n",
    "        # state size. ``(ngf*8) x 4 x 4``\n",
    "        nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(ngf * 4),\n",
    "        nn.ReLU(True),\n",
    "        # state size. ``(ngf*4) x 8 x 8``\n",
    "        nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(ngf * 2),\n",
    "        nn.ReLU(True),\n",
    "        # state size. ``(ngf*2) x 16 x 16``\n",
    "        nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(ngf),\n",
    "        nn.ReLU(True),\n",
    "        # state size. ``(ngf) x 32 x 32``\n",
    "        nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "        nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngpu = 1\n",
    "netG = Generator(ngpu).to(device)\n",
    "\n",
    "# Handle multi-GPU if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
    "\n",
    "# Apply the ``weights_init`` function to randomly initialize all weights\n",
    "#  to ``mean=0``, ``stdev=0.02``.\n",
    "netG.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
