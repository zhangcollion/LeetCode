{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n# /kaggle/input/ml2021spring-hw3/food-11/validation/07\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-29T01:29:52.825289Z","iopub.execute_input":"2022-05-29T01:29:52.825986Z","iopub.status.idle":"2022-05-29T01:29:52.831066Z","shell.execute_reply.started":"2022-05-29T01:29:52.825949Z","shell.execute_reply":"2022-05-29T01:29:52.830229Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Import necessary packages.\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom PIL import Image\n# \"ConcatDataset\" and \"Subset\" are possibly useful when doing semi-supervised learning.\nfrom torch.utils.data import ConcatDataset, DataLoader, Subset\nfrom torchvision.datasets import DatasetFolder\n\n# This is for the progress bar.\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-05-29T01:30:01.254008Z","iopub.execute_input":"2022-05-29T01:30:01.254418Z","iopub.status.idle":"2022-05-29T01:30:03.545187Z","shell.execute_reply.started":"2022-05-29T01:30:01.254364Z","shell.execute_reply":"2022-05-29T01:30:03.544232Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_tfm = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean = (0.5, 0.5, 0.5), std = (0.5, 0.5, 0.5)),\n#     transforms.CenterCrop((128, 128)),\n\n])\n\ntest_tfm = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean = (0.5, 0.5, 0.5), std = (0.5, 0.5, 0.5)),\n#     transforms.CenterCrop((128, 128)),\n\n])","metadata":{"execution":{"iopub.status.busy":"2022-05-29T01:30:03.888293Z","iopub.execute_input":"2022-05-29T01:30:03.889022Z","iopub.status.idle":"2022-05-29T01:30:03.895510Z","shell.execute_reply.started":"2022-05-29T01:30:03.888968Z","shell.execute_reply":"2022-05-29T01:30:03.894637Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\n\n# Construct datasets.\n# The argument \"loader\" tells how torchvision reads the data.\ntrain_set = DatasetFolder(\"/kaggle/input/ml2021spring-hw3/food-11/training/labeled\", loader=lambda x: Image.open(x), extensions=\"jpg\", transform=train_tfm)\nvalid_set = DatasetFolder(\"/kaggle/input/ml2021spring-hw3/food-11/validation\", loader=lambda x: Image.open(x), extensions=\"jpg\", transform=test_tfm)\nunlabeled_set = DatasetFolder(\"/kaggle/input/ml2021spring-hw3/food-11/training/unlabeled\", loader=lambda x: Image.open(x), extensions=\"jpg\", transform=train_tfm)\ntest_set = DatasetFolder(\"/kaggle/input/ml2021spring-hw3/food-11/testing\", loader=lambda x: Image.open(x), extensions=\"jpg\", transform=test_tfm)\n\n# Construct data loaders.\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\nvalid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\ntest_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T01:30:06.546096Z","iopub.execute_input":"2022-05-29T01:30:06.547008Z","iopub.status.idle":"2022-05-29T01:30:20.306640Z","shell.execute_reply.started":"2022-05-29T01:30:06.546967Z","shell.execute_reply":"2022-05-29T01:30:20.305816Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import torchvision.models as models\nclass Resnet(nn.Module):\n    def __init__(self, fc_hidden1=1024, fc_hidden2=768, drop_p=0.3, CNN_embed_dim=256):\n        super(Resnet, self).__init__()\n\n        self.fc_hidden1, self.fc_hidden2, self.CNN_embed_dim = fc_hidden1, fc_hidden2, CNN_embed_dim\n\n        # CNN architechtures\n        self.ch1, self.ch2, self.ch3, self.ch4 = 16, 32, 64, 128\n        self.k1, self.k2, self.k3, self.k4 = (5, 5), (3, 3), (3, 3), (3, 3)      # 2d kernal size\n        self.s1, self.s2, self.s3, self.s4 = (2, 2), (2, 2), (2, 2), (2, 2)      # 2d strides\n        self.pd1, self.pd2, self.pd3, self.pd4 = (0, 0), (0, 0), (0, 0), (0, 0)  # 2d padding\n\n        # encoding components\n        resnet = models.efficientnet_b4(pretrained=True)\n        modules = list(resnet.children())[:-1]      # delete the last fc layer.\n        self.resnet = nn.Sequential(*modules)\n        \n        \n        self.fc_layers = nn.Sequential(\n            nn.Linear(1792, 256),\n            nn.ReLU(),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            nn.Linear(256, 11)\n        )\n        \n    def forward(self, x):\n        x = self.resnet(x)   \n        x = x.view(x.size(0), -1)  # flatten output of conv\n        x = self.fc_layers(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-05-29T01:30:24.954762Z","iopub.execute_input":"2022-05-29T01:30:24.955340Z","iopub.status.idle":"2022-05-29T01:30:24.966163Z","shell.execute_reply.started":"2022-05-29T01:30:24.955306Z","shell.execute_reply":"2022-05-29T01:30:24.965438Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# class StudentNet(nn.Module):\n#     def __init__(self):\n#         super(StudentNet, self).__init__()\n\n#         self.cnn_layers = nn.Sequential(\n#             nn.Conv2d(3, 64, 3, 1, 1),\n#             nn.BatchNorm2d(64),\n#             nn.ReLU(),\n#             nn.MaxPool2d(2, 2, 0),\n\n#             nn.Conv2d(64, 128, 3, 1, 1),\n#             nn.BatchNorm2d(128),\n#             nn.ReLU(),\n#             nn.MaxPool2d(2, 2, 0),\n\n#             nn.Conv2d(128, 256, 3, 1, 1),\n#             nn.BatchNorm2d(256),\n#             nn.ReLU(),\n#             nn.MaxPool2d(4, 4, 0),\n#         )\n#         self.fc_layers = nn.Sequential(\n#             nn.Linear(256 * 8 * 8, 256),\n#             nn.ReLU(),\n#             nn.Linear(256, 256),\n#             nn.ReLU(),\n#             nn.Linear(256, 11)\n#         )\n\n#     def forward(self, x):\n#         x = self.cnn_layers(x)\n#         x = x.flatten(1)\n#         x = self.fc_layers(x)\n#         return x\n\nclass StudentNet(nn.Module):\n    def __init__(self):\n        super(StudentNet, self).__init__()\n\n        # ---------- TODO ----------\n        # Modify your model architecture\n\n        self.cnn = nn.Sequential(\n        nn.Conv2d(3, 32, 3), \n        nn.BatchNorm2d(32),\n        nn.ReLU(),\n        nn.Conv2d(32, 32, 3),  \n        nn.BatchNorm2d(32),\n        nn.ReLU(),\n        nn.MaxPool2d(2, 2, 0),     \n\n        nn.Conv2d(32, 64, 3), \n        nn.BatchNorm2d(64),\n        nn.ReLU(),\n        nn.MaxPool2d(2, 2, 0),     \n\n        nn.Conv2d(64, 100, 3), \n        nn.BatchNorm2d(100),\n        nn.ReLU(),\n        nn.MaxPool2d(2, 2, 0),\n\n        # Here we adopt Global Average Pooling for various input size.\n        nn.AdaptiveAvgPool2d((1, 1)),\n        )\n        self.fc = nn.Sequential(\n        nn.Linear(100, 11),\n        )\n      \n    def forward(self, x):\n        out = self.cnn(x)\n        out = out.view(out.size()[0], -1)\n        return self.fc(out)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T01:30:58.178198Z","iopub.execute_input":"2022-05-29T01:30:58.178990Z","iopub.status.idle":"2022-05-29T01:30:58.192248Z","shell.execute_reply.started":"2022-05-29T01:30:58.178952Z","shell.execute_reply":"2022-05-29T01:30:58.188918Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import torchvision.models as models\nclass Resnet_student(nn.Module):\n    def __init__(self, fc_hidden1=1024, fc_hidden2=768, drop_p=0.3, CNN_embed_dim=256):\n        super(Resnet_student, self).__init__()\n\n        self.fc_hidden1, self.fc_hidden2, self.CNN_embed_dim = fc_hidden1, fc_hidden2, CNN_embed_dim\n\n        # CNN architechtures\n        self.ch1, self.ch2, self.ch3, self.ch4 = 16, 32, 64, 128\n        self.k1, self.k2, self.k3, self.k4 = (5, 5), (3, 3), (3, 3), (3, 3)      # 2d kernal size\n        self.s1, self.s2, self.s3, self.s4 = (2, 2), (2, 2), (2, 2), (2, 2)      # 2d strides\n        self.pd1, self.pd2, self.pd3, self.pd4 = (0, 0), (0, 0), (0, 0), (0, 0)  # 2d padding\n\n        # encoding components\n        resnet = models.resnet18(pretrained=False)\n        modules = list(resnet.children())[:-1]      # delete the last fc layer.\n        self.resnet = nn.Sequential(*modules)\n        \n        \n        self.fc_layers = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            nn.Linear(256, 11)\n        )\n        \n    def forward(self, x):\n        x = self.resnet(x)   \n        x = x.view(x.size(0), -1)  # flatten output of conv\n        x = self.fc_layers(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-05-29T03:27:37.641903Z","iopub.execute_input":"2022-05-29T03:27:37.642266Z","iopub.status.idle":"2022-05-29T03:27:37.655351Z","shell.execute_reply.started":"2022-05-29T03:27:37.642236Z","shell.execute_reply":"2022-05-29T03:27:37.654318Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Initialize a model, and put it on the device specified.\nmodel = Resnet().to(device)\nmodel.device = device\n\n# For the classification task, we use cross-entropy as the measurement of performance.\ncriterion = nn.CrossEntropyLoss()\n\n# Initialize optimizer, you may fine-tune some hyperparameters such as learning rate on your own.\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-5)\n\n# The number of training epochs.\nn_epochs = 80\n\n# Whether to do semi-supervised learning.\ndo_semi = False","metadata":{"execution":{"iopub.status.busy":"2022-05-29T01:31:01.546176Z","iopub.execute_input":"2022-05-29T01:31:01.546574Z","iopub.status.idle":"2022-05-29T01:31:10.256157Z","shell.execute_reply.started":"2022-05-29T01:31:01.546542Z","shell.execute_reply":"2022-05-29T01:31:10.255233Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"concat_dataset = ConcatDataset([train_set, valid_set])\ntrain_loader = DataLoader(concat_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\nfor epoch in range(11):\n    # ---------- TODO ----------\n    # In each epoch, relabel the unlabeled dataset for semi-supervised learning.\n    # Then you can combine the labeled dataset and pseudo-labeled dataset for the training.\n#     if do_semi:\n#         # Obtain pseudo-labels for unlabeled data using trained model.\n#         pseudo_set = get_pseudo_labels(unlabeled_set, model)\n\n#         # Construct a new dataset and a data loader for training.\n#         # This is used in semi-supervised learning only.\n#         concat_dataset = ConcatDataset([train_set, pseudo_set])\n#         train_loader = DataLoader(concat_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n\n    # ---------- Training ----------\n    # Make sure the model is in train mode before training.\n    model.train()\n\n    # These are used to record information in training.\n    train_loss = []\n    train_accs = []\n\n    # Iterate the training set by batches.\n    for batch in tqdm(train_loader):\n\n        # A batch consists of image data and corresponding labels.\n        imgs, labels = batch\n\n        # Forward the data. (Make sure data and model are on the same device.)\n        logits = model(imgs.to(device))\n\n        # Calculate the cross-entropy loss.\n        # We don't need to apply softmax before computing cross-entropy as it is done automatically.\n        loss = criterion(logits, labels.to(device))\n\n        # Gradients stored in the parameters in the previous step should be cleared out first.\n        optimizer.zero_grad()\n\n        # Compute the gradients for parameters.\n        loss.backward()\n\n        # Clip the gradient norms for stable training.\n        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n\n        # Update the parameters with computed gradients.\n        optimizer.step()\n\n        # Compute the accuracy for current batch.\n        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n\n        # Record the loss and accuracy.\n        train_loss.append(loss.item())\n        train_accs.append(acc)\n\n    # The average loss and accuracy of the training set is the average of the recorded values.\n    train_loss = sum(train_loss) / len(train_loss)\n    train_acc = sum(train_accs) / len(train_accs)\n\n    # Print the information.\n    print(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n\n    # ---------- Validation ----------\n    # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n    model.eval()\n\n    # These are used to record information in validation.\n    valid_loss = []\n    valid_accs = []\n\n    # Iterate the validation set by batches.\n    for batch in tqdm(valid_loader):\n\n        # A batch consists of image data and corresponding labels.\n        imgs, labels = batch\n\n        # We don't need gradient in validation.\n        # Using torch.no_grad() accelerates the forward process.\n        with torch.no_grad():\n            logits = model(imgs.to(device))\n\n        # We can still compute the loss (but not the gradient).\n        loss = criterion(logits, labels.to(device))\n\n        # Compute the accuracy for current batch.\n        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n\n        # Record the loss and accuracy.\n        valid_loss.append(loss.item())\n        valid_accs.append(acc)\n\n    # The average loss and accuracy for entire validation set is the average of the recorded values.\n    valid_loss = sum(valid_loss) / len(valid_loss)\n    valid_acc = sum(valid_accs) / len(valid_accs)\n\n    # Print the information.\n    print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-29T01:31:17.832457Z","iopub.execute_input":"2022-05-29T01:31:17.833005Z","iopub.status.idle":"2022-05-29T01:39:59.566287Z","shell.execute_reply.started":"2022-05-29T01:31:17.832969Z","shell.execute_reply":"2022-05-29T01:39:59.565287Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# from torchsummary import summary\nstudent_net = Resnet_student().to(device)\nstudent_net.device = device\n\n\noptimizer_stu = torch.optim.Adam(student_net.parameters(), lr=0.0003, weight_decay=1e-5)\n\n# The number of training epochs.\nn_epochs = 30\nimport torch.nn.init as init\ndef weight_init(m):\n    '''\n    Usage:\n        model = Model()\n        model.apply(weight_init)\n    '''\n    print(f\"m is {m}\")\n    if isinstance(m, nn.Conv1d):\n        init.normal_(m.weight.data)\n        if m.bias is not None:\n            init.normal_(m.bias.data)\n    elif isinstance(m, nn.Conv2d):\n        init.xavier_normal_(m.weight.data)\n        if m.bias is not None:\n            init.normal_(m.bias.data)\n    elif isinstance(m, nn.Conv3d):\n        init.xavier_normal_(m.weight.data)\n        if m.bias is not None:\n            init.normal_(m.bias.data)\n    elif isinstance(m, nn.ConvTranspose1d):\n        init.normal_(m.weight.data)\n        if m.bias is not None:\n            init.normal_(m.bias.data)\n    elif isinstance(m, nn.ConvTranspose2d):\n        init.xavier_normal_(m.weight.data)\n        if m.bias is not None:\n            init.normal_(m.bias.data)\n    elif isinstance(m, nn.ConvTranspose3d):\n        init.xavier_normal_(m.weight.data)\n        if m.bias is not None:\n            init.normal_(m.bias.data)\n    elif isinstance(m, nn.BatchNorm1d):\n        init.normal_(m.weight.data, mean=1, std=0.02)\n        init.constant_(m.bias.data, 0)\n    elif isinstance(m, nn.BatchNorm2d):\n        init.normal_(m.weight.data, mean=1, std=0.02)\n        init.constant_(m.bias.data, 0)\n    elif isinstance(m, nn.BatchNorm3d):\n        init.normal_(m.weight.data, mean=1, std=0.02)\n        init.constant_(m.bias.data, 0)\n    elif isinstance(m, nn.Linear):\n        init.xavier_normal_(m.weight.data)\n        init.normal_(m.bias.data)\n    elif isinstance(m, nn.LSTM):\n        for param in m.parameters():\n            if len(param.shape) >= 2:\n                init.orthogonal_(param.data)\n            else:\n                init.normal_(param.data)\n    elif isinstance(m, nn.LSTMCell):\n        for param in m.parameters():\n            if len(param.shape) >= 2:\n                init.orthogonal_(param.data)\n            else:\n                init.normal_(param.data)\n    elif isinstance(m, nn.GRU):\n        for param in m.parameters():\n            if len(param.shape) >= 2:\n                init.orthogonal_(param.data)\n            else:\n                init.normal_(param.data)\n    elif isinstance(m, nn.GRUCell):\n        for param in m.parameters():\n            if len(param.shape) >= 2:\n                init.orthogonal_(param.data)\n            else:\n                init.normal_(param.data)\n                \nstudent_net.apply(weight_init)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T03:32:52.651936Z","iopub.execute_input":"2022-05-29T03:32:52.652535Z","iopub.status.idle":"2022-05-29T03:32:53.005552Z","shell.execute_reply.started":"2022-05-29T03:32:52.652495Z","shell.execute_reply":"2022-05-29T03:32:53.004702Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"## **How to implement?**\n* $Loss = \\alpha T^2 \\times KL(\\frac{\\text{Teacher's Logits}}{T} || \\frac{\\text{Student's Logits}}{T}) + (1-\\alpha)(\\text{Original Loss})$\n* Note that the logits here should have passed softmax.","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\ndef kl_loss(stu_pred, label, tea_pred, alpha=0.5):\n    T = 100\n    hard_loss = F.cross_entropy(stu_pred, label) * (1. - alpha)\n    soft_loss = nn.KLDivLoss()(F.log_softmax(stu_pred/T, dim=1),\n                             F.softmax(tea_pred/T, dim=1)) * (alpha * T * T) \n    return hard_loss+soft_loss","metadata":{"execution":{"iopub.status.busy":"2022-05-29T03:27:18.602823Z","iopub.execute_input":"2022-05-29T03:27:18.603187Z","iopub.status.idle":"2022-05-29T03:27:18.608967Z","shell.execute_reply.started":"2022-05-29T03:27:18.603160Z","shell.execute_reply":"2022-05-29T03:27:18.608215Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"concat_dataset = ConcatDataset([train_set, valid_set])\ntrain_loader = DataLoader(concat_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\nfor epoch in range(n_epochs):    \n    model.eval()\n    # These are used to record information in training.\n    train_loss = []\n    train_accs = []\n\n    # Iterate the training set by batches.\n    for batch in tqdm(train_loader):\n\n        # A batch consists of image data and corresponding labels.\n        imgs, labels = batch\n\n        # Forward the data. (Make sure data and model are on the same device.)\n        with torch.no_grad():\n            logits = model(imgs.to(device))\n            \n        student_logits = student_net(imgs.to(device))\n        loss = kl_loss(student_logits.to(device),labels.to(device), logits)\n\n        # Gradients stored in the parameters in the previous step should be cleared out first.\n        optimizer_stu.zero_grad()\n\n        # Compute the gradients for parameters.\n        loss.backward()\n\n        # Clip the gradient norms for stable training.\n        grad_norm = nn.utils.clip_grad_norm_(student_net.parameters(), max_norm=10)\n\n        # Update the parameters with computed gradients.\n        optimizer_stu.step()\n\n        # Compute the accuracy for current batch.\n        acc = (student_logits.argmax(dim=-1) == labels.to(device)).float().mean()\n\n        # Record the loss and accuracy.\n        train_loss.append(loss.item())\n        train_accs.append(acc)\n\n    # The average loss and accuracy of the training set is the average of the recorded values.\n    train_loss = sum(train_loss) / len(train_loss)\n    train_acc = sum(train_accs) / len(train_accs)\n\n    # Print the information.\n    print(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n\n    # ---------- Validation ----------\n    # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n    model.eval()\n\n    # These are used to record information in validation.\n    valid_loss = []\n    valid_accs = []\n\n    # Iterate the validation set by batches.\n    for batch in tqdm(valid_loader):\n\n        # A batch consists of image data and corresponding labels.\n        imgs, labels = batch\n\n        # We don't need gradient in validation.\n        # Using torch.no_grad() accelerates the forward process.\n        with torch.no_grad():\n            logits = student_net(imgs.to(device))\n\n        # We can still compute the loss (but not the gradient).\n        loss = criterion(logits, labels.to(device))\n\n        # Compute the accuracy for current batch.\n        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n\n        # Record the loss and accuracy.\n        valid_loss.append(loss.item())\n        valid_accs.append(acc)\n\n    # The average loss and accuracy for entire validation set is the average of the recorded values.\n    valid_loss = sum(valid_loss) / len(valid_loss)\n    valid_acc = sum(valid_accs) / len(valid_accs)\n\n    # Print the information.\n    print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-29T03:44:58.836431Z","iopub.execute_input":"2022-05-29T03:44:58.836785Z","iopub.status.idle":"2022-05-29T04:02:58.748670Z","shell.execute_reply.started":"2022-05-29T03:44:58.836756Z","shell.execute_reply":"2022-05-29T04:02:58.746357Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"student_net.eval()\n\n# Initialize a list to store the predictions.\npredictions = []\n\n# Iterate the testing set by batches.\nfor batch in tqdm(test_loader):\n\n    imgs, labels = batch\n\n    # We don't need gradient in testing, and we don't even have labels to compute loss.\n    # Using torch.no_grad() accelerates the forward process.\n    with torch.no_grad():\n        logits = student_net(imgs.to(device))\n\n    # Take the class with greatest logit as prediction and record it.\n    predictions.extend(logits.argmax(dim=-1).cpu().numpy().tolist())","metadata":{"execution":{"iopub.status.busy":"2022-05-29T03:16:38.397249Z","iopub.execute_input":"2022-05-29T03:16:38.398067Z","iopub.status.idle":"2022-05-29T03:17:08.066135Z","shell.execute_reply.started":"2022-05-29T03:16:38.398027Z","shell.execute_reply":"2022-05-29T03:17:08.065318Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"from torchsummary import summary\n# student_net = StudentNet()\nsummary(student_net, (3,128,128), device=\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-05-29T02:26:46.380072Z","iopub.execute_input":"2022-05-29T02:26:46.380713Z","iopub.status.idle":"2022-05-29T02:26:46.404523Z","shell.execute_reply.started":"2022-05-29T02:26:46.380678Z","shell.execute_reply":"2022-05-29T02:26:46.403437Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"sample = pd.DataFrame({\"Id\":np.arange(0,len(predictions)),\"Category\":predictions})\nsample.to_csv(\"ans_simple_model_distill_easy40v3.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T03:18:55.874547Z","iopub.execute_input":"2022-05-29T03:18:55.874961Z","iopub.status.idle":"2022-05-29T03:18:55.892316Z","shell.execute_reply.started":"2022-05-29T03:18:55.874929Z","shell.execute_reply":"2022-05-29T03:18:55.891424Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"sample","metadata":{"execution":{"iopub.status.busy":"2022-05-28T03:08:15.522192Z","iopub.execute_input":"2022-05-28T03:08:15.522828Z","iopub.status.idle":"2022-05-28T03:08:15.544158Z","shell.execute_reply.started":"2022-05-28T03:08:15.522796Z","shell.execute_reply":"2022-05-28T03:08:15.543291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}