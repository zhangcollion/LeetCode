{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
    "                               as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
    "\n",
    "tokenizer_pt = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized string is [7915, 1248, 7946, 7194, 13, 2799, 7877]\n",
      "The original string: Transformer is awesome.\n"
     ]
    }
   ],
   "source": [
    "sample_string = 'Transformer is awesome.'\n",
    "\n",
    "tokenized_string = tokenizer_en.encode(sample_string)\n",
    "print ('Tokenized string is {}'.format(tokenized_string))\n",
    "\n",
    "original_string = tokenizer_en.decode(tokenized_string)\n",
    "print ('The original string: {}'.format(original_string))\n",
    "\n",
    "assert original_string == sample_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(lang1, lang2):\n",
    "    lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(\n",
    "      lang1.numpy()) + [tokenizer_pt.vocab_size+1]\n",
    "\n",
    "    lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
    "      lang2.numpy()) + [tokenizer_en.vocab_size+1]\n",
    "\n",
    "    return lang1, lang2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
    "    return tf.logical_and(tf.size(x) <= max_length,\n",
    "                        tf.size(y) <= max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_encode(pt, en):\n",
    "    result_pt, result_en = tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\n",
    "    result_pt.set_shape([None])\n",
    "    result_en.set_shape([None])\n",
    "\n",
    "    return result_pt, result_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_examples.map(tf_encode)\n",
    "train_dataset = train_dataset.filter(filter_max_length)\n",
    "# 将数据集缓存到内存中以加快读取速度。\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "val_dataset = val_examples.map(tf_encode)\n",
    "val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # 将 sin 应用于数组中的偶数索引（indices）；2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # 将 cos 应用于数组中的奇数索引；2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)   \n",
    "#     print(matmul_qk.shape)\n",
    "    # 缩放 matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "#     print(scaled_attention_logits.shape)\n",
    " \n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "     \n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # \n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1.000000e+01, 9.276602e-25]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
       " array([[8.4332744e-26, 1.0000000e+00, 8.4332744e-26, 8.4332744e-26]],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "# 这条 `请求（query）符合第二个`主键（key）`，\n",
    "# 因此返回了第二个`数值（value）`。\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "scaled_dot_product_attention(temp_q, temp_k, temp_v,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        self.d_model =  d_model\n",
    "        self.n_head = n_heads\n",
    "        assert self.d_model%self.n_head == 0\n",
    "        self.depth = d_model // self.n_head\n",
    "        \n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.n_head, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "        \n",
    "        q = self.split_heads(q, batch_size)   \n",
    "        k = self.split_heads(k, batch_size)   \n",
    "        v = self.split_heads(v, batch_size) \n",
    "#         print(q.shape)\n",
    "#         print(k.shape)\n",
    "#         print(v.shape)\n",
    "#         print(q.shape)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "#         print(scaled_attention.shape)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0,2,1,3])\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))\n",
    "#         print(scaled_attention.shape)\n",
    "        output = self.dense(concat_attention)\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, n_heads=8)\n",
    "y = tf.random.uniform((3,60, 512))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([3, 60, 512]), TensorShape([3, 8, 60, 60]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),\n",
    "        tf.keras.layers.Dense(d_model)\n",
    "    ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_head, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha = MultiHeadAttention(d_model, n_head)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "        \n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        multi_output, _ = self.mha(x,x,x,mask)\n",
    "        drop_multi_output = self.dropout1(multi_output,training=training)\n",
    "        add_norm = self.layernorm1(x + drop_multi_output)\n",
    "        \n",
    "        ffn_data = self.ffn(add_norm)\n",
    "        drop_ffn_data= self.dropout2(ffn_data,training=training)\n",
    "        out_put = self.layernorm2(add_norm + drop_ffn_data)\n",
    "        \n",
    "        return out_put\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 60, 512])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "sample_encoder_layer_output = sample_encoder_layer(tf.random.uniform((32,60,512)), False, None)\n",
    "sample_encoder_layer_output.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_head, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.mask_mha = MultiHeadAttention(d_model, n_head)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.lay_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.mha = MultiHeadAttention(d_model, n_head)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.lay_norm2= tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "        self.lay_norm3= tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        mask_out, attn_weights_block1 = self.mask_mha(x,x,x,look_ahead_mask)\n",
    "        drop_mask_out = self.dropout1(mask_out, training=training)\n",
    "        norm_drop_mask_out = self.lay_norm1(x+drop_mask_out)\n",
    "        \n",
    "        mha_out, attn_weights_block2 = self.mha(enc_output,enc_output,norm_drop_mask_out, padding_mask)\n",
    "        drop_mha_out = self.dropout1(mha_out, training=training)\n",
    "        norm_drop_mha_out = self.lay_norm1(norm_drop_mask_out+drop_mha_out)\n",
    "        \n",
    "        ffn_out = self.ffn(norm_drop_mha_out)\n",
    "        drop_ffn_out = self.dropout3(ffn_out, training=training)\n",
    "        norm_drop_ffn_out = self.lay_norm3(norm_drop_mha_out+drop_ffn_out)\n",
    "        \n",
    "        return norm_drop_ffn_out, attn_weights_block1, attn_weights_block2\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 60, 512])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "sample_decoder_layer_output,_,_= sample_decoder_layer(tf.random.uniform((32,60,512)),sample_encoder_layer_output,False, None, None)\n",
    "sample_decoder_layer_output.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                            self.d_model)\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def  call(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x,training=training)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 62, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, input_vocab_size=8500,\n",
    "                         maximum_position_encoding=10000)\n",
    "\n",
    "sample_encoder_output = sample_encoder(tf.random.uniform((64, 62)), \n",
    "                                       training=False, mask=None)\n",
    "\n",
    "print(sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                 look_ahead_mask, padding_mask)\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer,  self).__init__()\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, \n",
    "                         dff, input_vocab_size,\n",
    "                         pe_input, rate)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                           target_vocab_size, pe_target, rate)\n",
    "        \n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "        \n",
    "    def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "        \n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)\n",
    "        \n",
    "        return final_output, attention_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 26, 8000])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
    "    input_vocab_size=8500, target_vocab_size=8000, \n",
    "    pe_input=10000, pe_target=6000)\n",
    "\n",
    "temp_input = tf.random.uniform((64, 62))\n",
    "temp_target = tf.random.uniform((64, 26))\n",
    "\n",
    "fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n",
    "                               enc_padding_mask=None, \n",
    "                               look_ahead_mask=None,\n",
    "                               dec_padding_mask=None)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = tokenizer_pt.vocab_size + 2\n",
    "target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'Train Step')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=input_vocab_size, \n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "    # 添加额外的维度来将填充加到\n",
    "    # 注意力对数（logits）。\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    # 编码器填充遮挡\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # 在解码器的第二个注意力模块使用。\n",
    "    # 该填充遮挡用于遮挡编码器的输出。\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # 在解码器的第一个注意力模块使用。\n",
    "    # 用于填充（pad）和遮挡（mask）解码器获取到的输入的后续标记（future tokens）。\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "#     print(look_ahead_mask.shape)\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 1, 1, 512)\n",
      "(55, 1, 512, 512)\n",
      "(60, 1, 1, 512)\n"
     ]
    }
   ],
   "source": [
    "a, b , c= create_masks(tf.random.uniform((60,512)), tf.random.uniform((55,512)))\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 1. ... 1. 1. 1.]\n",
      " [0. 0. 1. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 1. 1. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]], shape=(512, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(b[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# 如果检查点存在，则恢复最新的检查点。\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, \n",
    "                                     True, \n",
    "                                     enc_padding_mask, \n",
    "                                     combined_mask, \n",
    "                                     dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-d9fa47d6669c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# inp -> portuguese, tar -> english\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3024\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1961\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    597\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    # inp -> portuguese, tar -> english\n",
    "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "        train_step(inp, tar)\n",
    "\n",
    "    if batch % 1 == 0:\n",
    "        print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                         ckpt_save_path))\n",
    "\n",
    "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 40)\n",
      "(64, 39)\n"
     ]
    }
   ],
   "source": [
    "for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "    print(inp.shape)\n",
    "    print(tar.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, None), (None, None)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
