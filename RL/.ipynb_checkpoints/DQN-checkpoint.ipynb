{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T15:03:55.115561Z",
     "iopub.status.busy": "2021-06-16T15:03:55.114829Z",
     "iopub.status.idle": "2021-06-16T15:03:56.813338Z",
     "shell.execute_reply": "2021-06-16T15:03:56.812802Z"
    },
    "id": "tT4N3qYviUJr"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import gym\n",
    "import numpy as np\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# Set seed for experiment reproducibility\n",
    "seed = 42\n",
    "env.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Small epsilon value for stabilizing division operations\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T15:03:56.819537Z",
     "iopub.status.busy": "2021-06-16T15:03:56.818852Z",
     "iopub.status.idle": "2021-06-16T15:03:56.821009Z",
     "shell.execute_reply": "2021-06-16T15:03:56.820549Z"
    },
    "id": "aXKbbMC-kmuv"
   },
   "outputs": [],
   "source": [
    "class DQN(tf.keras.Model):\n",
    "    def __init__(self, num_actions: int, num_hidden_units: int):\n",
    "        super().__init__()\n",
    "        self.common1 = layers.Dense(128, activation=\"relu\")\n",
    "        self.common2 = layers.Dense(64, activation=\"relu\")\n",
    "        self.common3 = layers.Dense(32, activation=\"relu\")\n",
    "        self.actor = layers.Dense(num_actions)\n",
    "\n",
    "    def call(self, inputs: tf.Tensor):\n",
    "        x = self.common1(inputs)\n",
    "        x = self.common2(x)\n",
    "        x = self.common3(x)\n",
    "        return self.actor(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T15:03:56.825667Z",
     "iopub.status.busy": "2021-06-16T15:03:56.824118Z",
     "iopub.status.idle": "2021-06-16T15:03:58.392766Z",
     "shell.execute_reply": "2021-06-16T15:03:58.393155Z"
    },
    "id": "nWyxJgjLn68c"
   },
   "outputs": [],
   "source": [
    "num_actions = env.action_space.n  # 2\n",
    "num_hidden_units = 128\n",
    "model = DQN(num_actions, num_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T15:03:58.398778Z",
     "iopub.status.busy": "2021-06-16T15:03:58.398191Z",
     "iopub.status.idle": "2021-06-16T15:03:58.399964Z",
     "shell.execute_reply": "2021-06-16T15:03:58.400306Z"
    },
    "id": "5URrbGlDSAGx"
   },
   "outputs": [],
   "source": [
    "def env_step(action) :\n",
    "\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    return (state.astype(np.float32), \n",
    "          np.array(reward, np.int32), \n",
    "          done)\n",
    "\n",
    "\n",
    "def tf_env_step(action):\n",
    "    return tf.numpy_function(env_step, [action], [tf.float32, tf.int32, tf.bool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T15:03:58.407284Z",
     "iopub.status.busy": "2021-06-16T15:03:58.406713Z",
     "iopub.status.idle": "2021-06-16T15:03:58.408542Z",
     "shell.execute_reply": "2021-06-16T15:03:58.408896Z"
    },
    "id": "a4qVRV063Cl9"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def run_episode(initial_state: tf.Tensor,  model: tf.keras.Model, max_steps: int, gamma):\n",
    "\n",
    "    values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    rewards = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    returns = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "\n",
    "    initial_state_shape = initial_state.shape\n",
    "    state = initial_state\n",
    "\n",
    "    for t in tf.range(max_steps):\n",
    "        # Convert state into a batched tensor (batch size = 1)\n",
    "        state = tf.expand_dims(state, 0)\n",
    "        Qvalue = model(state)\n",
    "        action = tf.random.categorical(Qvalue, 1)[0, 0]\n",
    "        if np.random.rand() <= 1:\n",
    "            action = np.random.randint(2)\n",
    "            \n",
    "#         action = tf.math.argmax(Qvalue,axis=1)[0]\n",
    " \n",
    "        Qvalue = tf.reduce_max(Qvalue, axis=1)\n",
    "        values = values.write(t, Qvalue)\n",
    " \n",
    "        # Apply action to the environment to get next state and reward\n",
    "        state, reward, done = tf_env_step(action)\n",
    "        reward = tf.cast(reward, dtype=tf.float32)\n",
    "        next_value = reward\n",
    "        next_state = tf.expand_dims(state, 0)\n",
    "\n",
    "        state.set_shape(initial_state_shape)\n",
    "   \n",
    "        if not done:\n",
    "#             print(\"done----asddd-----------\", done)\n",
    "            next_Qvalue = model(next_state)\n",
    "            tmp_data = tf.reduce_max(next_Qvalue, axis=1)\n",
    "            next_value = reward +  gamma*tmp_data\n",
    "            next_value =  next_value[0]\n",
    " \n",
    "        returns =returns.write(t, next_value)\n",
    "        # Store reward\n",
    "        rewards = rewards.write(t, reward)\n",
    "        if tf.cast(done, tf.bool):\n",
    "            break\n",
    "\n",
    "    values = values.stack()\n",
    "    rewards = rewards.stack()\n",
    "    returns = returns.stack()\n",
    "    return values, rewards, returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.0337136, 1.002829 ], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### test\n",
    "initial_state = tf.constant(env.reset(), dtype=tf.float32)\n",
    "values, rewards, returns = run_episode(initial_state, model, max_steps=2, gamma=0.99)\n",
    "returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T15:03:58.420510Z",
     "iopub.status.busy": "2021-06-16T15:03:58.419951Z",
     "iopub.status.idle": "2021-06-16T15:03:58.421856Z",
     "shell.execute_reply": "2021-06-16T15:03:58.422199Z"
    },
    "id": "9EXwbEez6n9m"
   },
   "outputs": [],
   "source": [
    "# huber_loss = tf.keras.losses.mse()\n",
    "\n",
    "def compute_loss(values: tf.Tensor, returns: tf.Tensor):\n",
    "    critic_loss = tf.keras.losses.mse(values, returns)\n",
    "\n",
    "    return critic_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSYkQOmRfV75"
   },
   "source": [
    "### 4. Defining the training step to update parameters\n",
    "\n",
    "All of the steps above are combined into a training step that is run every episode. All steps leading up to the loss function are executed with the `tf.GradientTape` context to enable automatic differentiation.\n",
    "\n",
    "This tutorial uses the Adam optimizer to apply the gradients to the model parameters.\n",
    "\n",
    "The sum of the undiscounted rewards, `episode_reward`, is also computed in this step. This value will be used later on to evaluate if the success criterion is met.\n",
    "\n",
    "The `tf.function` context is applied to the `train_step` function so that it can be compiled into a callable TensorFlow graph, which can lead to 10x speedup in training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T15:03:58.428189Z",
     "iopub.status.busy": "2021-06-16T15:03:58.427588Z",
     "iopub.status.idle": "2021-06-16T15:03:58.429275Z",
     "shell.execute_reply": "2021-06-16T15:03:58.429616Z"
    },
    "id": "QoccrkF3IFCg"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "\n",
    "# @tf.function\n",
    "def train_step(\n",
    "    initial_state: tf.Tensor, \n",
    "    model: tf.keras.Model, \n",
    "    optimizer: tf.keras.optimizers.Optimizer, \n",
    "    gamma: float, \n",
    "    max_steps_per_episode: int)  :\n",
    "\n",
    " \n",
    "    with tf.GradientTape() as tape:\n",
    "        # Run the model for one episode to collect training data\n",
    "        values, rewards, returns = run_episode(\n",
    "            initial_state, model, max_steps_per_episode, gamma= 0.99)\n",
    "    \n",
    "        # Calculate expected returns\n",
    "#         returns = get_expected_return(rewards, gamma)\n",
    "\n",
    "        # Convert training data to appropriate TF tensor shapes\n",
    "        values, returns = [\n",
    "            tf.expand_dims(x, 1) for x in [ values, returns]] \n",
    "\n",
    "        # Calculating loss values to update our network\n",
    "        loss = compute_loss(values, returns)\n",
    "\n",
    "        # Compute the gradients from the loss\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "        # Apply the gradients to the model's parameters\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "#         print(\"------------reward？-------------\", rewards)\n",
    "\n",
    "        episode_reward = tf.math.reduce_sum(rewards)\n",
    "\n",
    "    return episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFvZiDoAflGK"
   },
   "source": [
    "### 5. Run the training loop\n",
    "\n",
    "Training is executed by running the training step until either the success criterion or maximum number of episodes is reached.  \n",
    "\n",
    "A running record of episode rewards is kept in a queue. Once 100 trials are reached, the oldest reward is removed at the left (tail) end of the queue and the newest one is added at the head (right). A running sum of the rewards is also maintained for computational efficiency. \n",
    "\n",
    "Depending on your runtime, training can finish in less than a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T15:03:58.434380Z",
     "iopub.status.busy": "2021-06-16T15:03:58.433810Z",
     "iopub.status.idle": "2021-06-16T15:05:13.702979Z",
     "shell.execute_reply": "2021-06-16T15:05:13.703412Z"
    },
    "id": "kbmBxnzLiUJx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 5521:  55%|▌| 5522/10000 [22:28<10:53,  6.85it/s, episode_reward=17, ru"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "min_episodes_criterion = 100\n",
    "max_episodes = 10000\n",
    "max_steps_per_episode = 1000\n",
    "\n",
    "# Cartpole-v0 is considered solved if average reward is >= 195 over 100 \n",
    "# consecutive trials\n",
    "reward_threshold = 195\n",
    "running_reward = 0\n",
    "\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "\n",
    "# Keep last episodes reward\n",
    "episodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)\n",
    "\n",
    "with tqdm.trange(max_episodes) as t:\n",
    "    for i in t:\n",
    "        initial_state = tf.constant(env.reset(), dtype=tf.float32)\n",
    "        episode_reward = int(train_step(\n",
    "            initial_state, model, optimizer, gamma, max_steps_per_episode))\n",
    "        episodes_reward.append(episode_reward)\n",
    "        running_reward = statistics.mean(episodes_reward)\n",
    "#         env.render()\n",
    "        t.set_description(f'Episode {i}')\n",
    "        t.set_postfix(\n",
    "            episode_reward=episode_reward, running_reward=running_reward)\n",
    "\n",
    "        # Show average episode reward every 10 episodes\n",
    "        if i % 10 == 0:\n",
    "            pass # print(f'Episode {i}: average reward: {avg_reward}')\n",
    "\n",
    "        if running_reward > reward_threshold and i >= min_episodes_criterion:  \n",
    "            break\n",
    "\n",
    "print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(episodes_reward)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "_jQ1tEQCxwRx"
   ],
   "name": "actor_critic.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
