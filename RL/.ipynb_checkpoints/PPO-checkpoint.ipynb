{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import json, random\n",
    "import pathlib\n",
    "import gym\n",
    "import collections\n",
    "import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorModel(tf.keras.Model):\n",
    "    def __init__(self, n_actions):\n",
    "        super(ActorModel, self).__init__()   \n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        self.fc1 = tf.keras.layers.Dense(256, activation=\"relu\")\n",
    "        self.fc2 = tf.keras.layers.Dense(256, activation=\"relu\")\n",
    "        self.fc3 = tf.keras.layers.Dense(self.n_actions, activation=\"softmax\")\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = tf.reshape(x, (1,4))\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CriticModel, self).__init__()\n",
    "        \n",
    "        self.fc1 = tf.keras.layers.Dense(256, activation=\"relu\")\n",
    "        self.fc2 = tf.keras.layers.Dense(256, activation=\"relu\")\n",
    "        self.fc3 = tf.keras.layers.Dense(1, activation=\"softmax\")\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = tf.reshape(x, (1,4))\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, max_length):\n",
    "        self.max_length = max_length\n",
    "        self.data = deque(maxlen=self.max_length)\n",
    "        self.data = []\n",
    "        \n",
    "    def memory(self, state, action, probs, vals, reward, next_state, done):\n",
    "        self.data.append((state, action, probs, vals, reward, next_state, done))\n",
    "   \n",
    "        \n",
    "    def replay(self, batch_size):\n",
    "        start = np.random.randint(0, len(self.data)-batch_size)\n",
    "#         minibatch = random.sample(self.data, batch_size)\n",
    "        minibatch = self.data[start: start+batch_size]\n",
    "        return minibatch\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, batch_size):\n",
    "        self.env = env\n",
    "        n_actions = self.env.action_space.n\n",
    "        self.gamma = 0.99\n",
    "        self.gae_lambda=0.95\n",
    "        self.policy_clip = 0.2\n",
    "        self.buffer = Buffer(10000)\n",
    "        self.actor = ActorModel(n_actions)\n",
    "        self.critic = CriticModel()\n",
    "        self.batch_size = batch_size\n",
    "        self.actor_opt = tf.optimizers.Adam(learning_rate=0.08)\n",
    "        self.critic_opt = tf.optimizers.Adam(learning_rate=0.08)\n",
    "        \n",
    "    def learn(self):\n",
    "        tmp_data = self.buffer.replay(self.batch_size)\n",
    "        \n",
    "        \n",
    "        states, actions, probs, vals, rewards, next_states, dones = [],[],[],[],[],[],[]\n",
    "        \n",
    "        for s, a, p, v, r, n, d in tmp_data:\n",
    "            states.append(s)\n",
    "            actions.append(a)\n",
    "            probs.append(p)\n",
    "            vals.append(v)\n",
    "            rewards.append(r)\n",
    "            next_states.append(n)\n",
    "            dones.append(d)\n",
    "            \n",
    "        advantages = []\n",
    "        for t in range(len(rewards)-1):\n",
    "            discount = 1\n",
    "            a_t = 0\n",
    "            for k in range(t, len(rewards)-1):\n",
    "                a_t += discount*(rewards[k] + self.gamma*vals[k+1]*(1-int(dones[k])) - vals[k])\n",
    "                discount *= self.gamma * self.gae_lambda \n",
    "            advantages.append(a_t)\n",
    "        advantages.append(tf.zeros((1,1)))\n",
    "            \n",
    "        for i in range(len(states)):\n",
    "            with tf.GradientTape() as tape:\n",
    "                dist = self.actor(states[i])\n",
    "                critic_value = self.critic(states[i])\n",
    "                new_probs = tf.math.log(tf.math.reduce_max(dist))\n",
    "                old_probs = probs[i]\n",
    "                prob_ratio = tf.math.exp(new_probs) / tf.math.exp(old_probs)\n",
    "                #prob_ratio = (new_probs - old_probs).exp()\n",
    "                weighted_probs = advantages[i] * prob_ratio\n",
    "                \n",
    "                weighted_clipped_probs = tf.clip_by_value(prob_ratio, 1-self.policy_clip,\n",
    "                        1+self.policy_clip)*advantages[i]\n",
    "#                 print(np.min(weighted_probs, weighted_clipped_probs))\n",
    "                actor_loss = -tf.math.reduce_min(tf.concat([weighted_probs, weighted_clipped_probs], axis=0))\n",
    "#                 actor_loss =  tf.math.reduce_mean(weighted_clipped_probs) \n",
    "            grads = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "            self.actor_opt.apply_gradients(zip(grads, self.actor.trainable_variables))\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                critic_value = self.critic(states[i])\n",
    "                returns = advantages[i] + vals[i]\n",
    "                critic_loss = (returns-critic_value)**2\n",
    "                critic_loss = tf.math.reduce_mean(critic_loss)         \n",
    "\n",
    "            grads = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "            self.critic_opt.apply_gradients(zip(grads, self.critic.trainable_variables))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def choose_action(self, state):\n",
    "   \n",
    "        probs = self.actor(state)\n",
    "        vals = self.critic(state)\n",
    "        action = tf.math.argmax(probs, axis=1)\n",
    "        prob = tf.math.log(tf.math.reduce_max(probs))\n",
    "        return action, prob, vals\n",
    "        \n",
    "    def train(self, episode_num=1000):\n",
    "        self.episode_num = episode_num\n",
    "        n_steps = 0\n",
    "        learn_times = 20\n",
    "        score = 0\n",
    "        for i in range(self.episode_num):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                action, probs, vals = self.choose_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action.numpy()[0])\n",
    "                self.buffer.memory(state, action, probs, vals, reward, next_state, done)\n",
    "                n_steps += 1\n",
    "                if n_steps % learn_times and n_steps > self.batch_size:\n",
    "                    self.learn()\n",
    "                state = next_state\n",
    "                score += reward\n",
    "            print(f\"{i} time is {score}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 time is 10.0\n",
      "1 time is 10.0\n",
      "2 time is 10.0\n",
      "3 time is 10.0\n",
      "4 time is 9.0\n",
      "5 time is 10.0\n",
      "6 time is 10.0\n",
      "7 time is 11.0\n",
      "8 time is 9.0\n",
      "9 time is 10.0\n",
      "10 time is 8.0\n",
      "11 time is 10.0\n",
      "12 time is 9.0\n",
      "13 time is 10.0\n",
      "14 time is 9.0\n",
      "15 time is 8.0\n",
      "16 time is 10.0\n",
      "17 time is 9.0\n",
      "18 time is 10.0\n",
      "19 time is 10.0\n",
      "20 time is 9.0\n",
      "21 time is 10.0\n",
      "22 time is 8.0\n",
      "23 time is 10.0\n",
      "24 time is 10.0\n",
      "25 time is 9.0\n",
      "26 time is 8.0\n",
      "27 time is 10.0\n",
      "28 time is 8.0\n",
      "29 time is 8.0\n",
      "30 time is 10.0\n",
      "31 time is 10.0\n",
      "32 time is 10.0\n",
      "33 time is 9.0\n",
      "34 time is 9.0\n",
      "35 time is 10.0\n",
      "36 time is 10.0\n",
      "37 time is 9.0\n",
      "38 time is 10.0\n",
      "39 time is 8.0\n",
      "40 time is 10.0\n",
      "41 time is 9.0\n",
      "42 time is 10.0\n",
      "43 time is 9.0\n",
      "44 time is 10.0\n",
      "45 time is 8.0\n",
      "46 time is 10.0\n",
      "47 time is 9.0\n",
      "48 time is 8.0\n",
      "49 time is 9.0\n",
      "50 time is 9.0\n",
      "51 time is 10.0\n",
      "52 time is 10.0\n",
      "53 time is 9.0\n",
      "54 time is 10.0\n",
      "55 time is 9.0\n",
      "56 time is 11.0\n",
      "57 time is 10.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "agent = Agent(env, batch_size=32)\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_min"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
