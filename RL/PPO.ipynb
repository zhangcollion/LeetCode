{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import json, random\n",
    "import pathlib\n",
    "import gym\n",
    "import collections\n",
    "import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AcModel(tf.keras.Model):\n",
    "    def __init__(self, n_actions):\n",
    "        super(AcModel, self).__init__()   \n",
    "        self.n_actions = n_actions\n",
    "        self.fc1 = tf.keras.layers.Dense(256, activation=\"relu\")\n",
    "        self.fc2 = tf.keras.layers.Dense(256, activation=\"relu\")\n",
    "        self.fc3 = tf.keras.layers.Dense(self.n_actions, activation=\"softmax\")\n",
    "        self.fc4 = tf.keras.layers.Dense(1, activation=\"linear\")\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = tf.convert_to_tensor([x], dtype=tf.float32)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        action_prob = self.fc3(x)\n",
    "        vals = self.fc4(x)\n",
    "        return action_prob, vals[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOMemory:\n",
    "    def __init__(self, batch_size):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_batches(self):\n",
    "        n_states = len(self.states)\n",
    "        batch_start = np.arange(0, n_states, self.batch_size)\n",
    "        indices = np.arange(n_states, dtype=np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "\n",
    "        return np.array(self.states),\\\n",
    "                np.array(self.actions),\\\n",
    "                np.array(self.probs),\\\n",
    "                np.array(self.vals),\\\n",
    "                np.array(self.rewards),\\\n",
    "                np.array(self.dones),\\\n",
    "                batches\n",
    "\n",
    "    def store_memory(self, state, action, probs, vals, reward, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.probs.append(probs)\n",
    "        self.vals.append(vals)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.vals = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, batch_size):\n",
    "        self.env = env\n",
    "        self.n_actions = self.env.action_space.n\n",
    "        self.gamma = 0.99\n",
    "        self.gae_lambda=0.95\n",
    "        self.policy_clip = 0.2\n",
    "#         self.actor = ActorModel(self.n_actions)\n",
    "#         self.critic = CriticModel()\n",
    "        self.ac = AcModel(self.n_actions)\n",
    "        self.batch_size = batch_size\n",
    "        self.ac_opt = tf.optimizers.Adam(learning_rate=0.08)\n",
    "#         self.critic_opt = tf.optimizers.Adam(learning_rate=0.08)\n",
    "        self.PPOMemory = PPOMemory(self.batch_size)\n",
    "        \n",
    "    def learn(self):\n",
    "        \n",
    "        states, actions, probs, vals, rewards, dones, batches_index = self.PPOMemory.generate_batches()   \n",
    "        advantage = np.zeros(len(rewards), dtype=np.float32)\n",
    "        for t in range(len(rewards)-1):\n",
    "            discount = 1\n",
    "            a_t = 0\n",
    "            for k in range(t, len(rewards)-1):\n",
    "                a_t += discount*(rewards[k] + self.gamma*vals[k+1]*(1-int(dones[k])) - vals[k])\n",
    "                discount *= self.gamma * self.gae_lambda \n",
    "            advantage[t] = a_t\n",
    "        for batch in batches_index:\n",
    "            new_probs = []\n",
    "            with tf.GradientTape() as tape:\n",
    "                dist, critic_value = self.ac(states[batch])\n",
    "                action = actions[batch]\n",
    "                for i, data in enumerate(dist[0]):\n",
    "                    new_probs.append(data[action[i][0]])\n",
    "                new_probs =  tf.convert_to_tensor(new_probs, dtype=tf.float32)\n",
    "                old_probs = probs[batch]\n",
    "                prob_ratio = tf.math.exp(new_probs) / tf.math.exp(old_probs)\n",
    "                weighted_probs = advantage[batch]* prob_ratio \n",
    "                weighted_clipped_probs = tf.clip_by_value(prob_ratio, 1-self.policy_clip,\n",
    "                        1+self.policy_clip)*advantage[batch]\n",
    "                weighted_probs = tf.reshape(weighted_probs, (weighted_probs.shape[0],1))\n",
    "                weighted_clipped_probs = tf.reshape(weighted_clipped_probs, (weighted_clipped_probs.shape[0],1))\n",
    "                tmp_data = tf.concat([weighted_probs, weighted_clipped_probs], axis=1)\n",
    "                actor_loss = tf.reduce_mean(-tf.math.reduce_min(tmp_data, axis=1))\n",
    "                tmp_adv = tf.reshape(advantage[batch], (advantage[batch].shape[0], 1))\n",
    "                returns = tmp_adv + vals[batch]\n",
    "                critic_loss_tmp = (returns-critic_value)**2\n",
    "                critic_loss = tf.math.reduce_mean(critic_loss_tmp)   \n",
    "                total_loss = 0.5*critic_loss + actor_loss\n",
    "            grads = tape.gradient(total_loss, self.ac.trainable_variables)\n",
    "            self.ac_opt.apply_gradients(zip(grads, self.ac.trainable_variables))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def choose_action(self, state):\n",
    "       \n",
    "        probs, vals = self.ac(state)\n",
    "        action = tf.math.argmax(probs, axis=1)\n",
    "        tmp_action = action.numpy()\n",
    "        prob = tf.math.log(probs[0, tmp_action[0]])\n",
    "        return action, prob, vals\n",
    "        \n",
    "    def train(self, episode_num=1000):\n",
    "        self.episode_num = episode_num\n",
    "        n_steps = 0\n",
    "        learn_times = 100\n",
    "        score = 0\n",
    "        for i in range(self.episode_num):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                action, probs, vals = self.choose_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action.numpy()[0])\n",
    "                self.PPOMemory.store_memory(state, action, probs, vals, reward, done)\n",
    "                n_steps += 1\n",
    "                if n_steps % learn_times and n_steps > self.batch_size:\n",
    "                    self.learn()\n",
    "                state = next_state\n",
    "                score += reward\n",
    "            print(f\"{i} time is {score}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------action---------- 0\n",
      "------action---------- 0\n",
      "------action---------- 1\n",
      "------action---------- 1\n",
      "------action---------- 1\n",
      "------action---------- 1\n",
      "------action---------- 0\n",
      "------action---------- 0\n",
      "------action---------- 0\n",
      "------action---------- 1\n",
      "------action---------- 0\n",
      "------action---------- 0\n",
      "------action---------- 0\n",
      "------action---------- 1\n",
      "------action---------- 0\n",
      "------action---------- 1\n",
      "------action---------- 0\n",
      "------action---------- 0\n",
      "------action---------- 0\n",
      "------action---------- 0\n",
      "0 time is 20.0\n",
      "------action---------- 1\n",
      "------action---------- 0\n",
      "------action---------- 0\n",
      "------action---------- 1\n",
      "------action---------- 1\n",
      "------action---------- 1\n",
      "------action---------- 1\n",
      "------action---------- 0\n",
      "------action---------- 1\n",
      "------action---------- 0\n",
      "------action---------- 0\n",
      "------action---------- 1\n",
      "------action---------- 0\n",
      "------action---------- 1\n",
      "------action---------- 0\n",
      "------action---------- 0\n",
      "------action---------- 1\n",
      "------action---------- 1\n",
      "------action---------- 0\n",
      "------action---------- 1\n",
      "------action---------- 0\n",
      "------action---------- 0\n",
      "------action---------- 1\n",
      "1 time is 23.0\n",
      "------action---------- 1\n",
      "------action---------- 0\n",
      "------action---------- 0\n",
      "------action---------- 0\n",
      "------action---------- 0\n",
      "------action---------- 0\n",
      "------action---------- 0\n",
      "------action---------- 1\n",
      "------action---------- 0\n",
      "------action---------- 0\n",
      "------action---------- 0\n",
      "------action---------- 0\n",
      "------action---------- 1\n",
      "------action---------- 1\n",
      "2 time is 14.0\n",
      "------action---------- 0\n",
      "------action---------- 0\n",
      "------action---------- 0\n",
      "------action---------- 0\n",
      "------action---------- 1\n",
      "------action---------- 0\n",
      "------action---------- 1\n",
      "------action---------- 0\n",
      "------action---------- 0\n",
      "------action---------- 0\n",
      "------action---------- 1\n",
      "------action---------- 1\n",
      "3 time is 12.0\n",
      "------action---------- 0\n",
      "------action---------- 1\n",
      "------action---------- 1\n",
      "------action---------- 0\n",
      "------action---------- 2\n",
      "------action----------\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "2 (<class 'numpy.int64'>) invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-119-acf0af6e8ec1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"CartPole-v0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-118-075b090654c7>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, episode_num)\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                 \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPPOMemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstore_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[0mn_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"%r (%s) invalid\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_dot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta_dot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: 2 (<class 'numpy.int64'>) invalid"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "agent = Agent(env, batch_size=64)\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "This is an alias of `random_sample`. See `random_sample`  for the complete\n",
       "documentation.\n",
       "\u001b[1;31mType:\u001b[0m      builtin_function_or_method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
