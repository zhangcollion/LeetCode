{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import json, random\n",
    "import pathlib\n",
    "import gym\n",
    "import collections\n",
    "import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AcModel(tf.keras.Model):\n",
    "    def __init__(self, n_actions):\n",
    "        super(AcModel, self).__init__()   \n",
    "        self.n_actions = n_actions\n",
    "        self.fc1 = tf.keras.layers.Dense(256, activation=\"relu\")\n",
    "        self.fc2 = tf.keras.layers.Dense(256, activation=\"relu\")\n",
    "        self.fc3 = tf.keras.layers.Dense(self.n_actions, activation=\"softmax\")\n",
    "        self.fc4 = tf.keras.layers.Dense(1, activation=\"linear\")\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = tf.convert_to_tensor([x], dtype=tf.float32)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        action_prob = self.fc3(x)\n",
    "        vals = self.fc4(x)\n",
    "        return action_prob, vals[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOMemory:\n",
    "    def __init__(self, batch_size):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_batches(self):\n",
    "        n_states = len(self.states)\n",
    "        batch_start = np.arange(0, n_states, self.batch_size)\n",
    "        indices = np.arange(n_states, dtype=np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "\n",
    "        return np.array(self.states),\\\n",
    "                np.array(self.actions),\\\n",
    "                np.array(self.probs),\\\n",
    "                np.array(self.vals),\\\n",
    "                np.array(self.rewards),\\\n",
    "                np.array(self.dones),\\\n",
    "                batches\n",
    "\n",
    "    def store_memory(self, state, action, probs, vals, reward, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.probs.append(probs)\n",
    "        self.vals.append(vals)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.vals = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, batch_size):\n",
    "        self.env = env\n",
    "        self.n_actions = self.env.action_space.n\n",
    "        self.gamma = 0.99\n",
    "        self.gae_lambda=0.95\n",
    "        self.policy_clip = 0.2\n",
    "#         self.actor = ActorModel(self.n_actions)\n",
    "#         self.critic = CriticModel()\n",
    "        self.ac = AcModel(self.n_actions)\n",
    "        self.batch_size = batch_size\n",
    "        self.ac_opt = tf.optimizers.Adam(learning_rate=0.08)\n",
    "#         self.critic_opt = tf.optimizers.Adam(learning_rate=0.08)\n",
    "        self.PPOMemory = PPOMemory(self.batch_size)\n",
    "        \n",
    "    def learn(self):\n",
    "        \n",
    "        states, actions, probs, vals, rewards, dones, batches_index = self.PPOMemory.generate_batches()   \n",
    "        advantage = np.zeros(len(rewards), dtype=np.float32)\n",
    "        for t in range(len(rewards)-1):\n",
    "            discount = 1\n",
    "            a_t = 0\n",
    "            for k in range(t, len(rewards)-1):\n",
    "                a_t += discount*(rewards[k] + self.gamma*vals[k+1]*(1-int(dones[k])) - vals[k])\n",
    "                discount *= self.gamma * self.gae_lambda \n",
    "            advantage[t] = a_t\n",
    "        for batch in batches_index:\n",
    "            new_probs = []\n",
    "            with tf.GradientTape() as tape:\n",
    "                dist, critic_value = self.ac(states[batch])\n",
    "                action = actions[batch]\n",
    "                for i, data in enumerate(dist[0]):\n",
    "                    new_probs.append(data[action[i][0]])\n",
    "                new_probs =  tf.convert_to_tensor(new_probs, dtype=tf.float32)\n",
    "                old_probs = probs[batch]\n",
    "                prob_ratio = tf.math.exp(new_probs) / tf.math.exp(old_probs)\n",
    "                weighted_probs = advantage[batch]* prob_ratio \n",
    "                weighted_clipped_probs = tf.clip_by_value(prob_ratio, 1-self.policy_clip,\n",
    "                        1+self.policy_clip)*advantage[batch]\n",
    "                weighted_probs = tf.reshape(weighted_probs, (weighted_probs.shape[0],1))\n",
    "                weighted_clipped_probs = tf.reshape(weighted_clipped_probs, (weighted_clipped_probs.shape[0],1))\n",
    "                tmp_data = tf.concat([weighted_probs, weighted_clipped_probs], axis=1)\n",
    "                actor_loss = tf.reduce_mean(-tf.math.reduce_min(tmp_data, axis=1))\n",
    "                tmp_adv = tf.reshape(advantage[batch], (advantage[batch].shape[0], 1))\n",
    "                returns = tmp_adv + vals[batch]\n",
    "                critic_loss_tmp = (returns-critic_value)**2\n",
    "                critic_loss = tf.math.reduce_mean(critic_loss_tmp)   \n",
    "                total_loss = 0.5*critic_loss + actor_loss\n",
    "            grads = tape.gradient(total_loss, self.ac.trainable_variables)\n",
    "            self.ac_opt.apply_gradients(zip(grads, self.ac.trainable_variables))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def choose_action(self, state):\n",
    "       \n",
    "        probs, vals = self.ac(state)\n",
    "        action = tf.math.argmax(probs, axis=1)\n",
    "        tmp_action = action.numpy()\n",
    "        prob = tf.math.log(probs[0, tmp_action[0]])\n",
    "        return action, prob, vals\n",
    "        \n",
    "    def train(self, episode_num=1000):\n",
    "        self.episode_num = episode_num\n",
    "        n_steps = 0\n",
    "        learn_times = 100\n",
    "        score = 0\n",
    "        for i in range(self.episode_num):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                action, probs, vals = self.choose_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action.numpy()[0])\n",
    "                self.PPOMemory.store_memory(state, action, probs, vals, reward, done)\n",
    "                n_steps += 1\n",
    "                if n_steps % learn_times and n_steps > self.batch_size:\n",
    "                    self.learn()\n",
    "                state = next_state\n",
    "                score += reward\n",
    "            print(f\"{i} time is {score}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 time is 9.0\n",
      "1 time is 9.0\n",
      "2 time is 38.0\n",
      "3 time is 14.0\n",
      "4 time is 25.0\n",
      "5 time is 9.0\n",
      "6 time is 10.0\n",
      "7 time is 20.0\n",
      "8 time is 15.0\n",
      "9 time is 12.0\n",
      "10 time is 15.0\n",
      "11 time is 23.0\n",
      "12 time is 24.0\n",
      "13 time is 24.0\n",
      "14 time is 25.0\n",
      "15 time is 29.0\n",
      "16 time is 26.0\n",
      "17 time is 11.0\n",
      "18 time is 56.0\n",
      "19 time is 72.0\n",
      "20 time is 62.0\n",
      "21 time is 55.0\n",
      "22 time is 41.0\n",
      "23 time is 32.0\n",
      "24 time is 39.0\n",
      "25 time is 57.0\n",
      "26 time is 30.0\n",
      "27 time is 30.0\n",
      "28 time is 30.0\n",
      "29 time is 17.0\n",
      "30 time is 29.0\n",
      "31 time is 33.0\n",
      "32 time is 25.0\n",
      "33 time is 38.0\n",
      "34 time is 33.0\n",
      "35 time is 11.0\n",
      "36 time is 40.0\n",
      "37 time is 24.0\n",
      "38 time is 22.0\n",
      "39 time is 46.0\n",
      "40 time is 31.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "agent = Agent(env, batch_size=64)\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "This is an alias of `random_sample`. See `random_sample`  for the complete\n",
       "documentation.\n",
       "\u001b[1;31mType:\u001b[0m      builtin_function_or_method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
