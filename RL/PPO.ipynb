{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import json, random\n",
    "import pathlib\n",
    "import gym\n",
    "import collections\n",
    "import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AcModel(tf.keras.Model):\n",
    "    def __init__(self, n_actions):\n",
    "        super(AcModel, self).__init__()   \n",
    "        self.n_actions = n_actions\n",
    "        self.fc1 = tf.keras.layers.Dense(256, activation=\"relu\")\n",
    "        self.fc2 = tf.keras.layers.Dense(256, activation=\"relu\")\n",
    "        self.fc3 = tf.keras.layers.Dense(self.n_actions, activation=\"softmax\")\n",
    "        self.fc4 = tf.keras.layers.Dense(1, activation=\"linear\")\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = tf.convert_to_tensor([x], dtype=tf.float32)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        actions = self.fc3(x)\n",
    "        vals = self.fc4(x)\n",
    "        return actions, vals[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOMemory:\n",
    "    def __init__(self, batch_size):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_batches(self):\n",
    "        n_states = len(self.states)\n",
    "        batch_start = np.arange(0, n_states, self.batch_size)\n",
    "        indices = np.arange(n_states, dtype=np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "\n",
    "        return np.array(self.states),\\\n",
    "                np.array(self.actions),\\\n",
    "                np.array(self.probs),\\\n",
    "                np.array(self.vals),\\\n",
    "                np.array(self.rewards),\\\n",
    "                np.array(self.dones),\\\n",
    "                batches\n",
    "\n",
    "    def store_memory(self, state, action, probs, vals, reward, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.probs.append(probs)\n",
    "        self.vals.append(vals)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.vals = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, batch_size):\n",
    "        self.env = env\n",
    "        self.n_actions = self.env.action_space.n\n",
    "        self.gamma = 0.99\n",
    "        self.gae_lambda=0.95\n",
    "        self.policy_clip = 0.2\n",
    "#         self.actor = ActorModel(self.n_actions)\n",
    "#         self.critic = CriticModel()\n",
    "        self.ac = AcModel(self.n_actions)\n",
    "        self.batch_size = batch_size\n",
    "        self.actor_opt = tf.optimizers.Adam(learning_rate=0.08)\n",
    "        self.critic_opt = tf.optimizers.Adam(learning_rate=0.08)\n",
    "        self.PPOMemory = PPOMemory(self.batch_size)\n",
    "        \n",
    "    def learn(self):\n",
    "        \n",
    "        states, actions, probs, vals, rewards, dones, batches_index = self.PPOMemory.generate_batches()\n",
    "            \n",
    "        advantage = np.zeros(len(rewards), dtype=np.float32)\n",
    "        for t in range(len(rewards)-1):\n",
    "            discount = 1\n",
    "            a_t = 0\n",
    "            for k in range(t, len(rewards)-1):\n",
    "                a_t += discount*(rewards[k] + self.gamma*vals[k+1]*(1-int(dones[k])) - vals[k])\n",
    "                discount *= self.gamma * self.gae_lambda \n",
    "            advantage[t] = a_t\n",
    "        \n",
    "\n",
    "        for batch in batches_index:\n",
    "            with tf.GradientTape() as tape:\n",
    "                dist, critic_value = self.ac(states[batch])\n",
    "                print(dist)\n",
    "                new_probs = tf.math.log(tf.math.reduce_max(dist[0, ]))\n",
    "                print(\"new_probs\", new_probs)\n",
    "                old_probs = probs[batch]\n",
    "                prob_ratio = tf.math.exp(new_probs) / tf.math.exp(old_probs)\n",
    "                #prob_ratio = (new_probs - old_probs).exp()\n",
    "                print('---prob_ratio--', prob_ratio)\n",
    "                weighted_probs = advantages[i] * prob_ratio\n",
    "                \n",
    "                weighted_clipped_probs = tf.clip_by_value(prob_ratio, 1-self.policy_clip,\n",
    "                        1+self.policy_clip)*advantages[i]\n",
    "#                 print(np.min(weighted_probs, weighted_clipped_probs))\n",
    "                actor_loss = -tf.math.reduce_min(tf.concat([weighted_probs, weighted_clipped_probs], axis=0))\n",
    "#                 actor_loss =  tf.math.reduce_mean(weighted_clipped_probs) \n",
    "            grads = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "            self.actor_opt.apply_gradients(zip(grads, self.actor.trainable_variables))\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                critic_value = self.critic(states[i])\n",
    "                returns = advantages[i] + vals[i]\n",
    "                critic_loss = (returns-critic_value)**2\n",
    "                critic_loss = tf.math.reduce_mean(critic_loss)         \n",
    "\n",
    "            grads = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "            self.critic_opt.apply_gradients(zip(grads, self.critic.trainable_variables))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def choose_action(self, state):\n",
    "       \n",
    "        probs, vals = self.ac(state)\n",
    "        action = tf.math.argmax(probs, axis=1)\n",
    "        tmp_action = action.numpy()\n",
    "        prob = tf.math.log(probs[0, tmp_action[0]])\n",
    "        return action, prob, vals\n",
    "        \n",
    "    def train(self, episode_num=1000):\n",
    "        self.episode_num = episode_num\n",
    "        n_steps = 0\n",
    "        learn_times = 100\n",
    "        score = 0\n",
    "        for i in range(self.episode_num):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                action, probs, vals = self.choose_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action.numpy()[0])\n",
    "                self.PPOMemory.store_memory(state, action, probs, vals, reward, done)\n",
    "                n_steps += 1\n",
    "                if n_steps % learn_times and n_steps > self.batch_size:\n",
    "                    self.learn()\n",
    "                state = next_state\n",
    "                score += reward\n",
    "            print(f\"{i} time is {score}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 time is 10.0\n",
      "1 time is 19.0\n",
      "2 time is 10.0\n",
      "3 time is 15.0\n",
      "4 time is 10.0\n",
      "tf.Tensor(\n",
      "[[[0.51268816 0.48731178]\n",
      "  [0.5022606  0.49773934]\n",
      "  [0.52461904 0.475381  ]\n",
      "  [0.5224362  0.4775638 ]\n",
      "  [0.5085549  0.49144506]\n",
      "  [0.51027375 0.48972622]\n",
      "  [0.5029648  0.49703515]\n",
      "  [0.50409555 0.49590445]\n",
      "  [0.51671726 0.48328272]\n",
      "  [0.5006018  0.4993981 ]\n",
      "  [0.5140161  0.48598394]\n",
      "  [0.5088054  0.49119467]\n",
      "  [0.50352484 0.49647516]\n",
      "  [0.50284153 0.49715844]\n",
      "  [0.51447296 0.4855271 ]\n",
      "  [0.4999527  0.50004727]\n",
      "  [0.51328033 0.4867197 ]\n",
      "  [0.5156003  0.4843997 ]\n",
      "  [0.49947566 0.5005243 ]\n",
      "  [0.5028746  0.49712536]\n",
      "  [0.514952   0.48504803]\n",
      "  [0.51116544 0.48883456]\n",
      "  [0.5095299  0.49047005]\n",
      "  [0.52218574 0.47781426]\n",
      "  [0.51611465 0.48388535]\n",
      "  [0.499723   0.50027704]\n",
      "  [0.5039852  0.4960148 ]\n",
      "  [0.51919734 0.48080266]\n",
      "  [0.5164462  0.48355383]\n",
      "  [0.5209348  0.47906518]\n",
      "  [0.5034054  0.49659458]\n",
      "  [0.50234455 0.49765545]\n",
      "  [0.503018   0.49698195]\n",
      "  [0.50505096 0.49494898]\n",
      "  [0.5038811  0.49611893]\n",
      "  [0.5204059  0.4795941 ]\n",
      "  [0.49989355 0.50010645]\n",
      "  [0.5187869  0.48121312]\n",
      "  [0.5176969  0.48230308]\n",
      "  [0.5044728  0.4955272 ]\n",
      "  [0.5229098  0.47709024]\n",
      "  [0.512082   0.48791802]\n",
      "  [0.5069996  0.49300033]\n",
      "  [0.49983698 0.500163  ]\n",
      "  [0.50771815 0.49228185]\n",
      "  [0.5082724  0.49172756]\n",
      "  [0.4995939  0.50040615]\n",
      "  [0.5034     0.49659994]\n",
      "  [0.5007972  0.49920276]\n",
      "  [0.5000039  0.49999613]\n",
      "  [0.51956576 0.48043427]\n",
      "  [0.50389606 0.4961039 ]\n",
      "  [0.5059444  0.49405566]\n",
      "  [0.49993753 0.5000624 ]\n",
      "  [0.5002168  0.49978322]\n",
      "  [0.50583893 0.49416104]\n",
      "  [0.5214481  0.4785519 ]\n",
      "  [0.513711   0.48628902]\n",
      "  [0.5183603  0.48163977]\n",
      "  [0.51882935 0.48117062]\n",
      "  [0.51149845 0.48850155]\n",
      "  [0.5171115  0.48288858]\n",
      "  [0.50558454 0.49441543]\n",
      "  [0.5207521  0.47924796]]], shape=(1, 64, 2), dtype=float32)\n",
      "new_probs tf.Tensor(\n",
      "[[-0.6680875  -0.6886361  -0.6450829  -0.6492524  -0.67618215 -0.67280793\n",
      "  -0.6872351  -0.68498945 -0.6602594  -0.69194424 -0.6655007  -0.6756897\n",
      "  -0.68612224 -0.6874802  -0.6646123  -0.69305265 -0.6669331  -0.6624234\n",
      "  -0.69209915 -0.6874144  -0.66368157 -0.671062   -0.67426676 -0.64973193\n",
      "  -0.66142637 -0.6925933  -0.6852083  -0.6554712  -0.6607842  -0.65213037\n",
      "  -0.68635947 -0.68846905 -0.68712926 -0.68309593 -0.68541497 -0.6531462\n",
      "  -0.6929343  -0.65626204 -0.6583653  -0.68424135 -0.64834625 -0.6692706\n",
      "  -0.67924505 -0.6928212  -0.6778288  -0.6767377  -0.69233525 -0.68637013\n",
      "  -0.691554   -0.69313943 -0.6547619  -0.6853853  -0.68132854 -0.6930224\n",
      "  -0.69271374 -0.681537   -0.6511456  -0.6660945  -0.6570847  -0.65618026\n",
      "  -0.67041075 -0.6594968  -0.68204004 -0.6524812 ]], shape=(1, 64), dtype=float32)\n",
      "---prob_ratio-- tf.Tensor(\n",
      "[[1.        1.        1.0000001 1.        1.        1.        1.\n",
      "  1.        1.        0.9999999 1.        1.        1.        1.\n",
      "  1.        1.        1.        1.        1.        1.        1.\n",
      "  1.        1.        0.9999999 1.0000001 1.        1.        1.\n",
      "  1.        1.        1.        0.9999999 1.        1.        1.\n",
      "  1.        1.        1.        1.        1.        1.        1.\n",
      "  1.        1.        1.        1.        1.        1.        1.\n",
      "  1.        1.        1.        1.        1.        1.        1.\n",
      "  1.        1.        1.        1.        1.        1.        1.\n",
      "  1.0000001]], shape=(1, 64), dtype=float32)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'advantages' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-acf0af6e8ec1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"CartPole-v0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-52-7626bde48de9>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, episode_num)\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[0mn_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mn_steps\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlearn_times\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mn_steps\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[0mscore\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-52-7626bde48de9>\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     38\u001b[0m                 \u001b[1;31m#prob_ratio = (new_probs - old_probs).exp()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'---prob_ratio--'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob_ratio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m                 \u001b[0mweighted_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madvantages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mprob_ratio\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                 weighted_clipped_probs = tf.clip_by_value(prob_ratio, 1-self.policy_clip,\n",
      "\u001b[1;31mNameError\u001b[0m: name 'advantages' is not defined"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "agent = Agent(env, batch_size=64)\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_min"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
