{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport numpy as np\nimport json, random\nimport pathlib\nimport gym\nimport collections\nimport tqdm\nfrom matplotlib import pyplot as plt\nfrom tensorflow.keras import layers\nfrom typing import Any, List, Sequence, Tuple\nfrom collections import deque","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-08T09:46:27.869231Z","iopub.execute_input":"2021-08-08T09:46:27.869802Z","iopub.status.idle":"2021-08-08T09:46:34.072856Z","shell.execute_reply.started":"2021-08-08T09:46:27.869693Z","shell.execute_reply":"2021-08-08T09:46:34.071942Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"seed = 42\ntf.random.set_seed(seed)\nnp.random.seed(seed)\neps = np.finfo(np.float32).eps.item()","metadata":{"execution":{"iopub.status.busy":"2021-08-08T09:46:34.074559Z","iopub.execute_input":"2021-08-08T09:46:34.074972Z","iopub.status.idle":"2021-08-08T09:46:34.080273Z","shell.execute_reply.started":"2021-08-08T09:46:34.074931Z","shell.execute_reply":"2021-08-08T09:46:34.079272Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class AcModel(tf.keras.Model):\n    def __init__(self, n_actions):\n        super(AcModel, self).__init__()   \n        self.n_actions = n_actions\n        self.fc1 = tf.keras.layers.Dense(256, activation=\"relu\")\n        self.fc2 = tf.keras.layers.Dense(256, activation=\"relu\")\n        self.fc3 = tf.keras.layers.Dense(self.n_actions, activation=\"softmax\")\n        self.fc4 = tf.keras.layers.Dense(1, activation=\"linear\")\n        \n    def call(self, x):\n        x = tf.convert_to_tensor([x], dtype=tf.float32)\n        x = self.fc1(x)\n        x = self.fc2(x)\n        actions = self.fc3(x)\n        vals = self.fc4(x)\n        return actions, vals[0]\n","metadata":{"execution":{"iopub.status.busy":"2021-08-08T09:46:34.084401Z","iopub.execute_input":"2021-08-08T09:46:34.084674Z","iopub.status.idle":"2021-08-08T09:46:34.093982Z","shell.execute_reply.started":"2021-08-08T09:46:34.084647Z","shell.execute_reply":"2021-08-08T09:46:34.093023Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class PPOMemory:\n    def __init__(self, batch_size):\n        self.states = []\n        self.probs = []\n        self.vals = []\n        self.actions = []\n        self.rewards = []\n        self.dones = []\n\n        self.batch_size = batch_size\n\n    def generate_batches(self):\n        n_states = len(self.states)\n        batch_start = np.arange(0, n_states, self.batch_size)\n        indices = np.arange(n_states, dtype=np.int64)\n        np.random.shuffle(indices)\n        batches = [indices[i:i+self.batch_size] for i in batch_start]\n\n        return np.array(self.states),\\\n                np.array(self.actions),\\\n                np.array(self.probs),\\\n                np.array(self.vals),\\\n                np.array(self.rewards),\\\n                np.array(self.dones),\\\n                batches\n\n    def store_memory(self, state, action, probs, vals, reward, done):\n        self.states.append(state)\n        self.actions.append(action)\n        self.probs.append(probs)\n        self.vals.append(vals)\n        self.rewards.append(reward)\n        self.dones.append(done)\n\n    def clear_memory(self):\n        self.states = []\n        self.probs = []\n        self.actions = []\n        self.rewards = []\n        self.dones = []\n        self.vals = []\n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-08T09:48:35.028673Z","iopub.execute_input":"2021-08-08T09:48:35.029103Z","iopub.status.idle":"2021-08-08T09:48:35.044993Z","shell.execute_reply.started":"2021-08-08T09:48:35.029071Z","shell.execute_reply":"2021-08-08T09:48:35.044059Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class Agent:\n    def __init__(self, env, batch_size):\n        self.env = env\n        self.n_actions = self.env.action_space.n\n        self.gamma = 0.99\n        self.gae_lambda=0.95\n        self.policy_clip = 0.2\n#         self.actor = ActorModel(self.n_actions)\n#         self.critic = CriticModel()\n        self.ac = AcModel(self.n_actions)\n        self.batch_size = batch_size\n        self.ac_opt = tf.optimizers.Adam(learning_rate=0.08)\n#         self.critic_opt = tf.optimizers.Adam(learning_rate=0.08)\n        self.PPOMemory = PPOMemory(self.batch_size)\n        \n    def learn(self):\n        \n        states, actions, probs, vals, rewards, dones, batches_index = self.PPOMemory.generate_batches()   \n        advantage = np.zeros(len(rewards), dtype=np.float32)\n        for t in range(len(rewards)-1):\n            discount = 1\n            a_t = 0\n            for k in range(t, len(rewards)-1):\n                a_t += discount*(rewards[k] + self.gamma*vals[k+1]*(1-int(dones[k])) - vals[k])\n                discount *= self.gamma * self.gae_lambda \n            advantage[t] = a_t\n        for batch in batches_index:\n            new_probs = []\n            with tf.GradientTape() as tape:\n                dist, critic_value = self.ac(states[batch])\n                action = actions[batch]\n                for i, data in enumerate(dist[0]):\n                    new_probs.append(data[action[i][0]])\n                new_probs =  tf.convert_to_tensor(new_probs, dtype=tf.float32)\n                old_probs = probs[batch]\n                prob_ratio = tf.math.exp(new_probs) / tf.math.exp(old_probs)\n                weighted_probs = advantage[batch]* prob_ratio \n                weighted_clipped_probs = tf.clip_by_value(prob_ratio, 1-self.policy_clip,\n                        1+self.policy_clip)*advantage[batch]\n                weighted_probs = tf.reshape(weighted_probs, (weighted_probs.shape[0],1))\n                weighted_clipped_probs = tf.reshape(weighted_clipped_probs, (weighted_clipped_probs.shape[0],1))\n                tmp_data = tf.concat([weighted_probs, weighted_clipped_probs], axis=1)\n                actor_loss = tf.reduce_mean(-tf.math.reduce_min(tmp_data, axis=1))\n                tmp_adv = tf.reshape(advantage[batch], (advantage[batch].shape[0], 1))\n                returns = tmp_adv + vals[batch]\n                critic_loss_tmp = (returns-critic_value)**2\n                critic_loss = tf.math.reduce_mean(critic_loss_tmp)   \n                total_loss = 0.5*critic_loss + actor_loss\n            grads = tape.gradient(total_loss, self.ac.trainable_variables)\n            self.ac_opt.apply_gradients(zip(grads, self.ac.trainable_variables))\n        \n        \n        \n        \n    def choose_action(self, state):\n       \n        probs, vals = self.ac(state)\n        action = tf.math.argmax(probs, axis=1)\n        tmp_action = action.numpy()\n        prob = tf.math.log(probs[0, tmp_action[0]])\n        return action, prob, vals\n        \n    \n        ","metadata":{"execution":{"iopub.status.busy":"2021-08-08T09:48:36.965261Z","iopub.execute_input":"2021-08-08T09:48:36.965959Z","iopub.status.idle":"2021-08-08T09:48:36.984452Z","shell.execute_reply.started":"2021-08-08T09:48:36.965919Z","shell.execute_reply":"2021-08-08T09:48:36.983466Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"env = gym.make(\"CartPole-v0\")\nbatch_size = 5\nagent = Agent(env, batch_size)\nepisode_num = 1000\nn_steps = 0\nlearn_times = 100\nscore = 0\nfor i in range(episode_num):\n    state = env.reset()\n    done = False\n    score = 0\n    while not done:\n        action, probs, vals = agent.choose_action(state)\n        next_state, reward, done, _ = env.step(action.numpy()[0])\n        agent.PPOMemory.store_memory(state, action, probs, vals, reward, done)\n        n_steps += 1\n        if n_steps % learn_times and n_steps > batch_size:\n            agent.learn()\n        state = next_state\n        score += reward\n    print(f\"{i} time is {score}\")\n","metadata":{"execution":{"iopub.status.busy":"2021-08-08T09:49:28.052099Z","iopub.execute_input":"2021-08-08T09:49:28.052695Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"0 time is 11.0\n1 time is 9.0\n2 time is 10.0\n3 time is 11.0\n4 time is 16.0\n5 time is 100.0\n6 time is 33.0\n7 time is 29.0\n8 time is 31.0\n9 time is 11.0\n10 time is 10.0\n11 time is 19.0\n12 time is 9.0\n13 time is 9.0\n14 time is 10.0\n15 time is 11.0\n16 time is 9.0\n17 time is 13.0\n18 time is 12.0\n19 time is 12.0\n20 time is 33.0\n21 time is 9.0\n22 time is 9.0\n23 time is 10.0\n24 time is 8.0\n25 time is 9.0\n26 time is 10.0\n27 time is 9.0\n28 time is 9.0\n29 time is 10.0\n30 time is 9.0\n31 time is 9.0\n32 time is 10.0\n33 time is 10.0\n34 time is 9.0\n35 time is 9.0\n36 time is 9.0\n37 time is 9.0\n38 time is 10.0\n39 time is 9.0\n40 time is 8.0\n41 time is 10.0\n42 time is 9.0\n43 time is 9.0\n44 time is 10.0\n45 time is 8.0\n46 time is 9.0\n47 time is 10.0\n48 time is 9.0\n49 time is 9.0\n50 time is 10.0\n51 time is 9.0\n52 time is 9.0\n53 time is 10.0\n54 time is 10.0\n55 time is 10.0\n56 time is 8.0\n57 time is 10.0\n58 time is 10.0\n59 time is 10.0\n60 time is 10.0\n61 time is 9.0\n62 time is 9.0\n63 time is 8.0\n64 time is 10.0\n65 time is 9.0\n66 time is 10.0\n67 time is 10.0\n68 time is 10.0\n69 time is 10.0\n70 time is 8.0\n71 time is 9.0\n72 time is 9.0\n73 time is 9.0\n74 time is 8.0\n75 time is 9.0\n76 time is 10.0\n77 time is 10.0\n78 time is 8.0\n79 time is 10.0\n80 time is 13.0\n81 time is 9.0\n82 time is 9.0\n83 time is 10.0\n84 time is 8.0\n85 time is 9.0\n86 time is 9.0\n87 time is 10.0\n88 time is 9.0\n89 time is 9.0\n90 time is 10.0\n91 time is 9.0\n92 time is 11.0\n93 time is 10.0\n94 time is 10.0\n95 time is 8.0\n96 time is 9.0\n97 time is 10.0\n98 time is 10.0\n99 time is 9.0\n100 time is 9.0\n101 time is 8.0\n102 time is 9.0\n103 time is 9.0\n104 time is 10.0\n105 time is 10.0\n106 time is 10.0\n107 time is 10.0\n108 time is 9.0\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}