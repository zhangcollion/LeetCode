{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-22T13:15:12.099683Z","iopub.execute_input":"2022-05-22T13:15:12.099954Z","iopub.status.idle":"2022-05-22T13:15:12.105940Z","shell.execute_reply.started":"2022-05-22T13:15:12.099926Z","shell.execute_reply":"2022-05-22T13:15:12.104937Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef no_axis_show(img, title='', cmap=None):\n  # imshow, and set the interpolation mode to be \"nearest\"。\n  fig = plt.imshow(img, interpolation='nearest', cmap=cmap)\n  # do not show the axes in the images.\n  fig.axes.get_xaxis().set_visible(False)\n  fig.axes.get_yaxis().set_visible(False)\n  plt.title(title)\n\ntitles = ['horse', 'bed', 'clock', 'apple', 'cat', 'plane', 'television', 'dog', 'dolphin', 'spider']\nplt.figure(figsize=(18, 18))\nfor i in range(10):\n  plt.subplot(1, 10, i+1)\n  fig = no_axis_show(plt.imread(f'/kaggle/input/ml2021spring-hw11/real_or_drawing/train_data/{i}/{500*i}.bmp'), title=titles[i])","metadata":{"execution":{"iopub.status.busy":"2022-05-22T13:15:15.221468Z","iopub.execute_input":"2022-05-22T13:15:15.221722Z","iopub.status.idle":"2022-05-22T13:15:15.860511Z","shell.execute_reply.started":"2022-05-22T13:15:15.221694Z","shell.execute_reply":"2022-05-22T13:15:15.859761Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Function\nimport cv2\n\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\n\nsource_transform = transforms.Compose([\n    # Turn RGB to grayscale. (Bacause Canny do not support RGB images.)\n    transforms.Grayscale(),\n    # cv2 do not support skimage.Image, so we transform it to np.array, \n    # and then adopt cv2.Canny algorithm.\n    transforms.Lambda(lambda x: cv2.Canny(np.array(x), 170, 300)),\n    # Transform np.array back to the skimage.Image.\n    transforms.ToPILImage(),\n    # 50% Horizontal Flip. (For Augmentation)\n    transforms.RandomHorizontalFlip(),\n    # Rotate +- 15 degrees. (For Augmentation), and filled with zero \n    # if there's empty pixel after rotation.\n    transforms.RandomRotation(15, fill=(0,)),\n    # Transform to tensor for model inputs.\n    transforms.ToTensor(),\n])\ntarget_transform = transforms.Compose([\n    # Turn RGB to grayscale.\n    transforms.Grayscale(),\n    # Resize: size of source data is 32x32, thus we need to \n    #  enlarge the size of target data from 28x28 to 32x32。\n    transforms.Resize((32, 32)),\n    # 50% Horizontal Flip. (For Augmentation)\n    transforms.RandomHorizontalFlip(),\n    # Rotate +- 15 degrees. (For Augmentation), and filled with zero \n    # if there's empty pixel after rotation.\n    transforms.RandomRotation(15, fill=(0,)),\n    # Transform to tensor for model inputs.\n    transforms.ToTensor(),\n])\n\nsource_dataset = ImageFolder('/kaggle/input/ml2021spring-hw11/real_or_drawing/train_data', transform=source_transform)\ntarget_dataset = ImageFolder('/kaggle/input/ml2021spring-hw11/real_or_drawing/test_data', transform=target_transform)\n\nsource_dataloader = DataLoader(source_dataset, batch_size=32, shuffle=True)\ntarget_dataloader = DataLoader(target_dataset, batch_size=32, shuffle=True)\ntest_dataloader = DataLoader(target_dataset, batch_size=128, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T13:15:19.832640Z","iopub.execute_input":"2022-05-22T13:15:19.833088Z","iopub.status.idle":"2022-05-22T13:16:00.020035Z","shell.execute_reply.started":"2022-05-22T13:15:19.833049Z","shell.execute_reply":"2022-05-22T13:16:00.019241Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class FeatureExtractor(nn.Module):\n    def __init__(self):\n        super(FeatureExtractor, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(1, 64, 3,1,1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            \n            nn.Conv2d(64, 128, 3,1,1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            \n            nn.Conv2d(128, 256, 3,1,1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            \n            nn.Conv2d(256, 256, 3,1,1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            \n            nn.Conv2d(256, 512, 3,1,1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(2),            \n        )\n        \n    def forward(self, x):\n        x = self.conv(x).squeeze()\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-05-22T13:16:00.021565Z","iopub.execute_input":"2022-05-22T13:16:00.021793Z","iopub.status.idle":"2022-05-22T13:16:00.032579Z","shell.execute_reply.started":"2022-05-22T13:16:00.021761Z","shell.execute_reply":"2022-05-22T13:16:00.030774Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class LabelPredictor(nn.Module):\n    def __init__(self):\n        super(LabelPredictor, self).__init__()\n        self.layer = nn.Sequential(\n            nn.Linear(512,512),\n            nn.ReLU(),\n            nn.Linear(512,512),\n            nn.ReLU(),\n            nn.Linear(512,10)\n        )\n        \n    def forward(self, h):\n        label_out = self.layer(h)\n        return label_out\n        ","metadata":{"execution":{"iopub.status.busy":"2022-05-22T13:18:51.123314Z","iopub.execute_input":"2022-05-22T13:18:51.123617Z","iopub.status.idle":"2022-05-22T13:18:51.134440Z","shell.execute_reply.started":"2022-05-22T13:18:51.123584Z","shell.execute_reply":"2022-05-22T13:18:51.133677Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class DomainClassifier(nn.Module):\n\n    def __init__(self):\n        super(DomainClassifier, self).__init__()\n\n        self.layer = nn.Sequential(\n            nn.Linear(512, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n\n            nn.Linear(512, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n\n            nn.Linear(512, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n\n            nn.Linear(512, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n\n            nn.Linear(512, 1),\n        )\n\n    def forward(self, h):\n        y = self.layer(h)\n        return y","metadata":{"execution":{"iopub.status.busy":"2022-05-22T13:16:59.605780Z","iopub.execute_input":"2022-05-22T13:16:59.606035Z","iopub.status.idle":"2022-05-22T13:16:59.613612Z","shell.execute_reply.started":"2022-05-22T13:16:59.606007Z","shell.execute_reply":"2022-05-22T13:16:59.612920Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2022-05-22T13:16:59.615139Z","iopub.execute_input":"2022-05-22T13:16:59.616344Z","iopub.status.idle":"2022-05-22T13:16:59.626537Z","shell.execute_reply.started":"2022-05-22T13:16:59.616302Z","shell.execute_reply":"2022-05-22T13:16:59.625833Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"feature_extractor = FeatureExtractor().to(device)\nlabel_predictor = LabelPredictor().to(device)\ndomain_classifier = DomainClassifier().to(device)\n\nclass_criterion = nn.CrossEntropyLoss()\ndomain_criterion = nn.BCEWithLogitsLoss()\n\noptim_F = optim.Adam(feature_extractor.parameters(), lr=1e-3)\noptim_C = optim.Adam(label_predictor.parameters(), lr=1e-3)\noptim_D = optim.Adam(domain_classifier.parameters(), lr=1e-3)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T13:18:55.512666Z","iopub.execute_input":"2022-05-22T13:18:55.512917Z","iopub.status.idle":"2022-05-22T13:18:55.557635Z","shell.execute_reply.started":"2022-05-22T13:18:55.512889Z","shell.execute_reply":"2022-05-22T13:18:55.556904Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def train_epoch(source_dataloader, target_dataloader, lamb):\n    running_D_loss, running_F_loss = 0.0, 0.0\n    total_hit, total_num = 0.0, 0.0\n    for i, ((src_data, src_label),(tgt_data, _)) in enumerate(zip(source_dataloader, target_dataloader)):\n        src_data = src_data.to(device)\n        src_label = src_label.to(device)\n        tgt_data = tgt_data.to(device)\n        \n        mix_data = torch.cat((src_data, tgt_data), dim=0)\n        domain_label = torch.zeros([src_data.shape[0]+tgt_data.shape[0], 1]).to(device)\n        domain_label[:src_data.shape[0]] = 1\n        \n        feature = feature_extractor(mix_data)\n        domain_logits = domain_classifier(feature.detach())\n        loss = domain_criterion(domain_logits, domain_label)\n        running_D_loss += loss.item()\n        loss.backward()\n        optim_D.step()\n        \n        class_logits = label_predictor(feature[:src_data.shape[0]])\n        domain_logits = domain_classifier(feature)\n        loss = class_criterion(class_logits, src_label) - lamb*domain_criterion(domain_logits, domain_label)\n        running_F_loss += loss.item()\n        loss.backward()\n        optim_F.step()\n        optim_C.step()\n        \n        optimizer_D.zero_grad()\n        optimizer_F.zero_grad()\n        optimizer_C.zero_grad()\n        \n        total_hit += torch.sum(torch.argmax(class_logits, dim=1) == source_label).item()\n        total_num += source_data.shape[0]\n        print(i, end='\\r')\n    \n    return running_D_loss / (i+1), running_F_loss / (i+1), total_hit / total_num","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in  range(200):\n    train_D_loss, train_F_loss, train_acc = train_epoch(source_dataloader, target_dataloader, lamb=0.1)\n\n    torch.save(feature_extractor.state_dict(), f'extractor_model.pt')\n    torch.save(label_predictor.state_dict(), f'predictor_model.pt')\n    pritn(f\"{epoch} epoch train D loss {train_D_loss :6.4f}, train F loss {train_F_loss :6.4f}, train acc {train_acc :6.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"presult = []\nlabel_predictor.eval()\nfeature_extractor.eval()\nfor i, (test_data, _) in enumerate(test_dataloader):\n    test_data = test_data.to(device)\n    class_logits = label_predictor(feature_extractor(test_data))\n    x = torch.argmax(class_logits, dim=1).cpu().detach().numpy()\n    result.append(x)\n\nresult = np.concatenate(result)\ndf = pd.DataFrame({\"id\":np.arrage(0, len(result)), \"label\":result})\ndf.to_csv(\"sample.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}